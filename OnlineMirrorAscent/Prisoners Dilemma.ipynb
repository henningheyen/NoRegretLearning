{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(\"/Users/henningheyen/Desktop/Studium/TUM/BachelorThesis/Git/NoRegretLearning/Screenshots/PrisonersDilemmaWikipedia.png\", width = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations\n",
    "T = int(100)\n",
    "\n",
    "#number of players\n",
    "n = 2\n",
    "\n",
    "#number of strategies (C: Confess, S: Stay Silent) \n",
    "d = 2\n",
    "\n",
    "#Lipschitz Constant\n",
    "L = 1\n",
    "\n",
    "#step size \n",
    "eta = np.sqrt(np.log(d))/(L*np.sqrt(2*T))\n",
    "#eta = 0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######### Payoff Matrix #########\n",
    "A = np.array([[-1,-3],[0,-2]])\n",
    "\n",
    "# payoff matrix player 2:\n",
    "B = A.transpose()\n",
    "\n",
    "print(\"A: \\n\" + str(A))\n",
    "print(\"B: \\n\" + str(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Inizialization #########\n",
    "p1_Silent = 0.9\n",
    "p2_Silent = 0.9\n",
    "strategy1 = np.array([p1_Silent, 1-p1_Silent])  \n",
    "strategy2 = np.array([p2_Silent, 1-p2_Silent]) \n",
    "\n",
    "print(\"Strategy Player 1: \\n\" + str(strategy1))\n",
    "print(\"Strategy Player 2: \\n\" + str(strategy2))\n",
    "\n",
    "\n",
    "# list to store results\n",
    "p1, p2 = [p1_Silent], [p2_Silent]\n",
    "\n",
    "########## Online Gradient Ascent Algorithm #############\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    # compute gradient\n",
    "    grad1 = A.dot(strategy2)\n",
    "    grad2 = strategy1.transpose().dot(B) \n",
    "\n",
    "    # update strategy (OMA with entropic regularizer)\n",
    "    strategy1 = strategy1*np.exp(eta*grad1)/sum(strategy1*np.exp(eta*grad1))\n",
    "    strategy2 = strategy2*np.exp(eta*grad2)/sum(strategy2*np.exp(eta*grad2))\n",
    "\n",
    "    # store p_Head in result vector p\n",
    "    p1 += [strategy1[0]]\n",
    "    p2 += [strategy2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.scatter(p1_Silent, p2_Silent, color='tab:blue',marker='o',s=70, zorder=1, label='initial strategy')\n",
    "plt.scatter([0], [0], color='red',marker='*',s=100, label='Nash Equilibrium')\n",
    "plt.plot(p1,p2, marker = '*')\n",
    "\n",
    "plt.xlabel('P(Stay Silent) Player 1', fontsize=15)\n",
    "plt.ylabel('P(Stay Silent) Player 2', fontsize=15)\n",
    "plt.title('Prisoners Dilemma', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=14)\n",
    "plt.ylim(0,1); plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
