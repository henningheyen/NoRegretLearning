
\chapter{No-Regret Learning}\label{chapter:noRegretLearning}

Regret dynamics are one the best-studied algorithmic frameworks in online convex optimization. This chapter will introduce the concept of no-regret learning, similar to \cite[Chapter 2]{HDRmertikopoulos}. Later we will cover two concrete no-regret algorithms, namely \textit{projected online gradient descent} and \textit{entropic gradient descent}. In the simulations of chapter \ref{chapter:simulations}, we will then run those two algorithms on simple two-player games. Note that throughout chapter \ref{chapter:noRegretLearning}, we will minimize losses, which is the convention in optimization. In chapter \ref{chapter:finiteGames}, on the other hand, we are maximizing utilities, which is the convention in game theory.


\section{Notation and Basic Definitions}\label{section:notationAndDefinitionsRegret}

Let us first define some basic concepts that we will make use of. Throughout sets are denoted by upper case letters and vectors by lower case letters. In online learning, a sequence of play is considered. Therefore, we denote $x_t$ the $t$-th vector in the sequence of $x_1, \dots, x_T$ where $T$ is the number of iterations. In a slight abuse of notation, we will also use index notation for the $i$-th element of a vector, but it will be clear from the context. \\

The \textit{inner product} of two vectors $x$ and $y$ with dimension $n$ is defined as 

\begin{equation*}
    \langle x,y\rangle = \sum_{i=1}^{n}x_i y_i
\end{equation*}

The \textit{norm} $l_p$ of a vector $x$ is defined as

\begin{equation*}
    \|x\|_p = (\sum_{i}(|x_i|^p)^{1/p}
\end{equation*}

In particular, the $l_2$ (or \textit{Euclidean}) norm is then $\|x\|_2 = \sqrt{\langle x,x\rangle}$. \\

The \textit{gradient} of a differentiable function $f: \mathcal{X} \to \mathbb{R}$ is denoted by $\nabla f$. A function $f$ is called $L-$\textit{Lipschitz} over a set $\mathcal{X}$ with respect to some norm $\|\cdot\|$ if for all $x,y \in \mathcal{X}$ we have that, 

\begin{equation*}
    |f(x) - f(y)| \le L\|x - y\|
\end{equation*}

A function $f:\mathcal{X} \to \mathbb{R}$ is said to be \textit{convex} if for all $x,y$ and $\lambda \in [0,1]$ we have that 

\begin{equation*}
    f(\lambda x + (1-\lambda)y) \le \lambda f(x) + (1-\lambda)f(y)
\end{equation*}

Likewise, a function $f:\mathcal{X} \to \mathbb{R}$ is said to be \textit{concave} if for all $x,y$ and $\lambda \in [0,1]$ we have that 

\begin{equation*}
    f(\lambda x + (1-\lambda)y) \ge \lambda f(x) + (1-\lambda)f(y)
\end{equation*}

A function $f:\mathcal{X} \to \mathbb{R}$ is $\sigma-$\textit{strongly-convex} over $\mathcal{X}$ with respect to some norm $\|\cdot\|$ if there is some $x \in \mathcal{X}$ such that for all $y \in \mathcal{X}$ we have 

\begin{equation*}
    f(y) \ge f(x) + \langle\nabla f(x),y - x\rangle + \frac{\sigma}{2}\|y - x\|^2
\end{equation*}

A set is called \textit{convex} if for all $x,y \in \mathcal{X}$ and $\lambda \in [0,1]$ we have that 
\begin{equation*}
    \lambda x + (1-\lambda)y \in \mathcal{X} 
\end{equation*}

Throughout this paper, $\mathcal{V}$ will denote a finite-dimensional real space with some norm $\|\cdot\|$ and $\mathcal{X} \subseteq \mathcal{V}$ a closed convex subset thereof. We will write ri($\mathcal{X}$) as the relative interior of $\mathcal{X}$ and diam($\mathcal{X}$) $ = $ sup$\{\|x'-x\|: x,x' \in \mathcal{X}\}$) for its diameter. Also, we will denote $\mathcal{Y} \equiv \mathcal{V}^*$ for the algebraic dual of $\mathcal{V}$, $\langle y,x \rangle$ for the canonical pairing of $y \in \mathcal{Y}$ and $x \in \mathcal{V}$ and $\|y\|_* \equiv $ sup$\{\langle y,x \rangle: \|x\| \le 1\}$ for its dual norm. Given an extended real-valued function $f: \mathcal{V} \to \mathbb{R} \cup \{+\infty\}$ its effective domain is defined as dom$f = \{x \in \mathcal{V} : f(x) < \infty\}$.


\section{The Basic Model}\label{section:theBasicModel}

In online optimization, the goal is to minimize the aggregate loss incurred against a sequence of unknown loss functions. Formally, at every time step $t = 1, \dots T$, the optimizer selects an \textit{action} $x_t$ from a closed convex subset $\mathcal{X}$ of an $n$-dimensional normed space $\mathcal{V}$. After that, the optimizer suffers a convex loss $l_t(x_t)$ based on an a priori unknown loss function $l_t:\mathcal{X} \to \mathbb{R}$. Based on that, the optimizer then updates its action and repeats. See figure \ref{fig:OCO} for a pseudo-code description.

\begin{figure}[H]\centering
    %\textit{Online Convex Optimization}
    \begin{minipage}{.9\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \KwInput{convex action set $\mathcal{X}$, sequence of convex loss functions $l_t:\mathcal{X} \to \mathbb{R}$ }
        \For{$t = 1,2,\dots,$T} {
        select action $x_t \in \mathcal{X}$ \;
        incur loss $l_t(x_t)$ \;
        update $x_t \gets x_{t+1}$ \;
        }
        \end{algorithm}\caption{pseudo-code for the online convex optimization framework}  \label{fig:OCO}
  \end{minipage}
\end{figure}

Based on the properties of the loss function, we distinguish between two disjoint subclasses of online optimization. Throughout, we assume $l_t$ to be differentiable and that it attains a minimum in $\mathcal{X}$.

\begin{enumerate}
    \item\label{item:stronglyOptimization} \textit{Online strongly convex optimization}: each $l_t$ is assumed to be $\alpha_t$-strongly convex, i.e. with respect to $\|\cdot\|$ there is some $x \in \mathcal{X}$ such that for all $x' \in \mathcal{X}$  we have that 
    \begin{equation}\label{equ:stronglyOptimization}
        l_t(x') \ge l_t(x) + \langle \nabla l_t(x),x'-x\rangle + \frac{\alpha_t}{2}\|x'-x\|^2
    \end{equation}
    for some $\alpha_t > 0$
    \item \textit{Online linear optimization}: each $l_t$ is assumed to be linear, i.e. 
    \begin{equation}\label{equ:linaerOptimization}
        l_t(x) = -\langle v_t,x\rangle
    \end{equation}
    for some \textit{payoff vector} $v_t \in \mathcal{V}^*$
\end{enumerate}

Depending on the information available to the optimizer, we can specify two feedback assumptions:

\begin{enumerate}
    \item \textit{Full information}: the entire loss function $l_t$ is revealed to the optimizer at each time step.
    \item \textit{First order information}: only the perfect gradient $\nabla l_t(x_t)$ for some input $x_t \in \mathcal{X}$ is revealed. 
\end{enumerate}

Obviously, \textit{first order information} is a much lighter assumption than \textit{full information} and therefore applicable to a broader range of problems. 


\section{Regret Minimization}\label{section:regretMinimization}

The notion of \textit{regret} is a widely used performance measure of online algorithms. In words, it measures how "sorry" the optimizer is not to have followed
a fixed competing action in hindsight. It is the difference between the cumulative loss
of the actual sequence of play induced by an algorithm and the cumulative loss of the best fixed action.

\begin{definition}\label{def:regret}
    The regret incurred by a sequence of \textit{actions} $x_t \in \mathcal{X}$ and a sequence of loss functions $l_t:\mathcal{X} \to\mathbb{R}$ for an algorithm running for $T$ iterations is defined as
    \[reg(T) = \sum_{t=1}^T l_t(x_t) - \min_{x \in \mathcal{X}} \sum_{t=1}^T l_t(x)\]
\end{definition}

The learner's goal is to have the lowest regret possible. In fact, we say an algorithm exhibits \textit{no-regret} if the regret tends to zero as $T$, the number of iterations, goes to infinity.

\begin{definition}\label{def:noRegret}
    An algorithm exhibits no-regret iff $reg(T)$ grows sublinearly  with $T$, i.e.
    \[reg(T) = o(T)\]
\end{definition}

The fundamental question in online convex optimization is whether no-regret is achievable. We will address this in the following section. 

\section{Leader Following Policies}\label{section:LeaderFollowingPolocies}

The first no-regret candidate is based on the simple update rule: at time $t+1$, select the optimal action in hindsight up to including time step $t$. This policy is known as \textit{follow the leader} (FTL). 

\begin{equation}
    \tag{FTL}
    x_{t+1} = \argmin_{x \in \mathcal{X}} \sum_{i=1}^t l_i(x)
    \label{equ:FTL}
\end{equation} \\

This approach, however, needs \textit{full information feedback}, i.e. the knowledge of the entire loss function $l_t$ once $x_t$ is chosen, plus the ability to compute the argmin operator in the update step. Both requirements are much lighter in \textit{online linear optimization}, where loss functions are in the form of equation \ref{equ:linaerOptimization}. But even for \textit{online linear optimization} problems, the no-regret property is not guaranteed under \ref{equ:FTL}. Consider the following example. \\

Let $\mathcal{X} = [-1,1]$ and set the linear loss function as follows

\begin{equation*}
    l_t(x) = \begin{cases}
    -x/2 &\text{if $t = 1$}\\
    x &\text{if $t$ is even}\\
    -x &\text{if $t > 1 \land t$ is odd}
    \end{cases}
\end{equation*} \\

Apart from $t = 1$, where $x_t$ could actually be set arbitrarily in $[-1,1]$ according to \ref{equ:FTL} policy we have that $x_t = (-1)^t$ for all $t > 1$. The incurred loss after $T$ iterations is then in the form of $\sum_{t=1}^T l_t(x_t) = T - x_1/2 -1$. Comparing that with the fixed action $x_t = 0$ for all $t$, we have that $reg(T) \sim T$, which is not sublinear with $T$. We can conclude that \ref{equ:FTL} does not guarantee no-regret. \\

In some sense, \ref{equ:FTL} seems to be "unstable". The predictions shift drastically from round to round. One way to stabilize \ref{equ:FTL} is to add a so-called regularization (or penalty) term. It makes sure that the prediction in the upcoming round is not too "far" off from the current one. That leads to a policy known as \textit{follow the regularized leader} (\ref{equ:FTRL}). It can be formulated as follows

\begin{equation}
    \tag{FTRL}
    x_{t+1} = \argmin_{x \in \mathcal{X}} \bigg\{\sum_{i=1}^t l_i(x) + \frac{1}{\gamma}h(x)\bigg\}
    \label{equ:FTRL}
\end{equation} \\

The regularization function is denoted by $h: \mathcal{X} \to \mathbb{R}$, and $\gamma > 0$ is a tunable step size parameter that adjusts the weight on the regularization term. In order to obtain a stabilizing effect, it is common to assume that $h$ is $K$-strongly convex and continuous. Considering the regret analysis of \ref{equ:FTRL} we have the following result \cite[Theorem 2.1]{HDRmertikopoulos}.

\begin{proposition}\label{prop:RegretFTRL}
    Suppose \ref{equ:FTRL} is run on a sequence of $l_1,\dots,l_T$ of convex loss functions. Further, assume each $l_t$ is $L_t$-Lipschitz with respect to some norm $\|\cdot\|$ and $L \equiv \textnormal{sup}_t L_t < \infty$. Let $H \equiv \textnormal{max } h-\textnormal{min } h$ be the "depth" of $h$ and assume $h$ to be $K$-strongly convex and set $\gamma = \frac{1}{L}\sqrt{\frac{HK}{T}}$. Then we have that
    \[reg(T) \le 2L\sqrt{(H/K)T} = o(T)\]
\end{proposition} 

Proposition \ref{prop:RegretFTRL} shows that under some assumptions, no-regret is indeed achievable. These include that \textit{(i)} the optimizer has full information on the entire loss function up to the current time step, \textit{(ii)} the minimization problem in the \ref{equ:FTRL} update rule can be solved efficiently, and \textit{(iii)} the horizon of play is known in advance. While \textit{(iii)} can be resolved easily by the \textit{doubling trick} method \cite{shalev}, the other two are much harder to overcome. The easiest way to minimize a loss function that requires only \textit{first order information} is based on an algorithm known as \textit{(projected) online gradient descent}.


\section{Projected Online Gradient Descent}\label{section:ProjectedOnlineGradienDescent}

The most straightforward method to minimize a loss function in optimization theory is based on gradient descent dynamics. The algorithm simply takes a step in the direction of the objective's gradient. Then, if the problem is constrained, the result is projected back to the feasible region, which is the probability simplex in finite games with mixed extensions. The process repeats. This policy is known as \textit{projected online gradient descent} (\ref{equ:POGD}) and can be formulated by the following recursive update rule

\begin{equation}
   \tag{POGD}
        x_{t+1} = \Pi(x_t + \gamma_t v_t)
   \label{equ:POGD}
\end{equation}

where

\begin{equation}
    v_t = -\nabla_t = -\nabla l_t(x_t)
    \label{equ:gradient}
\end{equation}

denotes the gradient of the loss function at $x_t$, $\gamma_t > 0 $ it the step size and $\Pi: \mathcal{V} \to \mathcal{X}$ is the \textit{Euclidean projector} 

\begin{equation*}
    \Pi(x) = \argmin_{x'\in \mathcal{X}}\|x'-x\|_2^{2}
\end{equation*}

For a pseudo-code description and a schematic representation\footnote{borrowed from \cite[Chapter 2]{HDRmertikopoulos}} of \ref{equ:POGD} see figure \ref{fig:POGDpseudoCodeAndScheme}.

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
    \begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{step size sequence $\gamma_t > 0$}
    \For{$t = 1,2,\dots,$T} {
    incur loss $l_t(x_t)$ \;
    receive feedback $v_t \gets -\nabla l_t(x_t)$ \;
    update $x_{t+1} = \Pi(x_t + \gamma_t v_t)$ \;
    }
    \end{algorithm}
    %\caption{pseudo-code for \ref{equ:POGD}}
    %\label{fig:POGDpseudocode}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{logos/POGDscheme.png}
  %\captionof{figure}{schematic representation of \ref{equ:POGD}}
  %\label{fig:POGDscheme}
\end{subfigure}
\caption{pseudo-code and schematic representation of \ref{equ:POGD}}
\label{fig:POGDpseudoCodeAndScheme}
\end{figure}

It is worth mentioning that the addition $x_t + \gamma_t v_t$ in the update step is not well-defined in general, as we add a primal with a dual vector. Assuming $\mathcal{V}$ to be the Euclidean space, however, we have that its dual space $\mathcal{V}^*$ is canonically identified with $\mathcal{V}$. This assumption can only be made when the Euclidean norm is used \cite{HDRmertikopoulos}. Let us move to the regret analysis of \ref{equ:POGD} \cite[Theorem 2.2]{HDRmertikopoulos}.

\begin{proposition}\label{prop:regretPOGD}
    Suppose \ref{equ:POGD} is run on a sequence of convex and $L_t$-Lipschitz loss functions $l_t$ with a constant step size $\gamma_t \equiv \gamma > 0$ where  \textnormal{diam}($\mathcal{X}$) $\equiv$ \textnormal{max}$\{\|x'-x\|_2:x,x' \in \mathcal{X}\}$ denotes the diameter of $\mathcal{X}$. In particular, if $L \equiv \textnormal{sup}_t L_t < \infty$ and $\gamma = \frac{\textnormal{diam}(\mathcal{X})}{L\sqrt{T}}$, then the regret is bounded by 
    \[reg(T) \le \textnormal{diam}(\mathcal{X})L\sqrt{T}\]
\end{proposition}

We can conclude that up to a multiplicative constant, \ref{equ:POGD} enjoys the same regret bound as \ref{equ:FTRL} (proposition \ref{prop:RegretFTRL}). Both algorithms have regret in the size of $\mathcal{O}(\sqrt{T})$ and thereby, both are no-regret algorithms. \ref{equ:FTRL}, however, needs \textit{full information} on the loss function $l_t$ at each time step, whereas \ref{equ:POGD} just needs \textit{first-order information} in the form of the gradient. That makes \ref{equ:POGD} more lightweight and applicable to a wide range of problems \cite{HDRmertikopoulos}. \\

Before we move on let us address the connection between \textit{follow the regularized leader} and \textit{online gradient descent}. Consider the unconstrained linear optimization problem with $\mathcal{X} = \mathbb{R}^n$, Euclidean regularizer $h = \frac{1}{2}\|x\|_2^2$ and linear losses of the form $l_t(x) = -\langle v_t,x\rangle$ for some sequence $v_t \in \mathbb{R}^n$. According to \ref{equ:FTRL} this yields \cite{HDRmertikopoulos}

\begin{equation*}
    \begin{split}
        x_{t+1} & = \argmin_{x \in \mathcal{X}} \bigg\{\sum_{i=1}^t l_i(x) + \frac{1}{\gamma}h(x)\bigg\} = \argmin_{x \in \mathbb{R}^n} \bigg\{\|x\|_2^2 - 2\gamma\sum_{i=1}^t \langle v_i,x\rangle\bigg\} \\
        & = \argmin_{x \in \mathbb{R}^n} \big\|x - \gamma\sum_{i=1}^t \langle v_i,x\rangle\big\|_2^2 = \gamma\sum_{i=1}^t v_i = \gamma\sum_{i=1}^{t-1} v_i + \gamma v_t = x_t + \gamma v_t
    \end{split}
\end{equation*} \\

That is simply the unprojected update policy of \ref{equ:POGD}. It is an example of a much more general link between \textit{leader following} and \textit{gradient} dynamics. The main idea is to "linearize" \ref{equ:FTRL} in the sense that we replace $l_t(x)$ with its \textit{linear surrogate} $\Tilde{l}(x)$.

\begin{equation*}
    \Tilde{l}_t(x) = l_t(x_t) + \langle \nabla l_t(x_t),x_t-x \rangle
\end{equation*}

Applying the linear surrogate to \ref{equ:FTRL} leads to \textit{follow the linearized leader} \ref{equ:FTLL} \cite{HDRmertikopoulos}. 

\begin{equation}
    \tag{FTLL}
    x_{t+1} = \argmax_{x \in \mathcal{X}} \bigg\{\gamma\sum_{i=1}^t v_s - h(x)\bigg\}
    \label{equ:FTLL}
\end{equation}

The signal $v_s$ simply denotes the perfect gradient of the loss function $l_t$ at $x_t$ analog to equation \ref{equ:gradient}. Note that by this modification \ref{equ:FTLL} only requires first order information, just like \ref{equ:POGD}. Writing \ref{equ:FTLL} recursively leads to the \textit{dual averaging} framework. For more details on that, refer to \cite{HDRmertikopoulos, mertikopoulos}.


\section{Online Mirror Descent}\label{section:OnlineMirrorDescent}

As discussed in \cite{shalev, HDRmertikopoulos}, there are cases where the problem's underlying geometry may allow considerably sharper regret bounds as in \ref{equ:POGD}. The reason is that the Lipschitz constant $L$ is a multiplicative factor in the regret, and it depends on the underlying norm. In \ref{equ:POGD}, we constrain ourselves on the $l_2$ norm. Allowing an adaptive Lipschitz constant, however, can be beneficial. For instance, in the \textit{multi-armed bandit} problem, when using the Euclidean norm, the Lipschitz constant $L$ can be bounded by $\sqrt{n}$. Using $l_\infty$ norm instead of $l_2$ norm, however, we can bound $L$ by $1$. For a more detailed survey, refer to \cite{shalev, HDRmertikopoulos}. The natural question arises whether running \ref{equ:POGD} with non-Euclidean norm can lead to better regret bounds. The \textit{online mirror descent} framework addresses that question.\\

To understand the idea of online mirror descent, let us revisit projected online gradient descent and formulate it more abstractly. Given an input point $x \gets x_t$ and an impulsive vector $y \gets \gamma_t v_t$, then \ref{equ:POGD} returns an output point $x^+ \gets x_{t+1}$ defined as

\begin{equation}
\begin{split}
x^+ = \Pi(x + y) & = \argmin_{x' \in \mathcal{X}}\bigg\{\|x + y - x'\|_2^2\bigg\} \\
 & = \argmin_{x' \in \mathcal{X}}\bigg\{\|x - x'\|_2^2 + \|y\|_2^2 + 2\langle y,x-x'\rangle\bigg\} \\
 & = \argmin_{x' \in \mathcal{X}}\bigg\{\langle y,x-x'\rangle + D(x',x)\bigg\}
\end{split}
\label{equ:EuclideanProj}
\end{equation}

where 

\begin{equation*}
    D(x',x) = \frac{1}{2}\|x'-x\|_2^2 = \frac{1}{2}\|x'\|_2^2 - \frac{1}{2}\|x\|_2^2 - \langle x,x'-x\rangle
\end{equation*}

is the squared Euclidean distance between x and x'. The function $D: \mathcal{X}\times\mathcal{X} \to \mathbb{R}$ is called \textit{Bregman divergence} and can be generalized by 

\begin{equation*}
    D(x',x) = h(x') - h(x) - \langle\nabla h(x), x'-x\rangle
\end{equation*}

The Bregman divergence is induced by some regularization function $h: \mathcal{X} \to \mathbb{R}$. The basic idea of online mirror descent is to replace the Euclidean distance in \ref{equ:POGD} with some Bregman divergence induced by a regularization function that is not necessarily based on the Euclidean norm. By that, we can make use of the problem's geometric properties and hope for a better regret bound. The regularization function can be viewed analog to the one introduced in \ref{equ:FTRL}, so again, we assume $h$ to be continuous and $K$-strongly convex with respect to some norm $\|\cdot\|$ \cite{HDRmertikopoulos}. \\

Depending on the Bregman divergence $D$ induced by some regularization function $h$ we can define a more general projection $Q:\mathcal{X}\times\mathcal{Y} \to \mathcal{X}$ named \textit{mirror map} as 

\begin{equation*}
    Q_x(y) = \argmin_{x'\in \mathcal{X}}\bigg\{\langle y,x-x'\rangle + D(x',x)\bigg\} \qquad \forall x\in \mathcal{X}, y \in \mathcal{Y}
\end{equation*}

Finally, we can formulate the online mirror descent (\ref{equ:OMD}) update policy.

\begin{equation*}
    \tag{OMD}
    x_{t+1} = Q_{x_t}(\gamma_t v_t)
    \label{equ:OMD}
\end{equation*}

where $\gamma_t > 0$ again is the step size sequence and $v_t = -\nabla l_t(x_t)$ is the first-order information of the loss functions $l_t$ in the form of the gradient. Note that the mirror map $Q$ is solely induced by the choice of $h$, the regularization function. Considering the regret analysis of \ref{equ:OMD}, we have the following result \cite[Theorem 2.4]{HDRmertikopoulos}

\begin{proposition}\label{prop:regretOMD}
    Suppose \ref{equ:OMD} is run on a sequence of $l_1,\dots,l_T$ of convex loss functions. Further, assume each $l_t$ is $L_t$-Lipschitz with respect to some norm $\|\cdot\|$ and $L \equiv \textnormal{sup}_t L_t < \infty$. Let $H \equiv \textnormal{max } h-\textnormal{min } h$ be the "depth" of $h$ and assume $h$ to be $K$-strongly convex and set $\gamma = \frac{1}{L}\sqrt{\frac{2HK}{T}}$. Then we have that
    \[reg(T) \le L\sqrt{(2H/K)T}\]
\end{proposition}

In terms of regret bounds, the main difference between \ref{equ:POGD} (proposition \ref{prop:regretOMD} and \ref{equ:OMD} (proposition \ref{prop:regretPOGD}) is the factor $2H/K$ and the norm defining the Lipschitz constant $L$. The factor fully depends on the choice of the regularizer $h$. So adjusting $h$ to the problem may lead to improved efficiency. Let us introduce two examples of regularization functions and their corresponding mirror maps. \\

Consider the \textit{Euclidean regularizer}.

\begin{equation*}
    h(x) = \frac{1}{2}\|x\|_2^2
\end{equation*}

Obviously, the induced mirror map is simply the Euclidean projection in the form of equation \ref{equ:EuclideanProj}. So, in that case, \ref{equ:OMD} coincides with \ref{equ:POGD}. 

\begin{equation*}
    Q_x(y) = \argmin_{x'\in \mathcal{X}}\bigg\{\langle y,x-x'\rangle + \frac{1}{2}\|x'-x\|_2^2\bigg\} = \Pi(x + y)
\end{equation*}

Another commonly used regularization function is the \textit{entropic regularizer}. Let $\mathcal{X} = \Delta(\mathcal{A})$ be the standard probability simplex of $\mathbb{R}^n$. The entropic regularization function is defined as

\begin{equation*}
    h(x) = \sum_{a\in\mathcal{A}}x_a\textnormal{log}(x_a)
\end{equation*}

A straightforward calculation shows that $h$ is $1$-strongly convex with respect to the $l_1$ norm \cite{HDRmertikopoulos}. The induced mirror map can be written as

\begin{equation*}
    Q_x(y) = \frac{(x_a\textnormal{exp}(y_a))_{a\in\mathcal{A}}}{\sum_{a\in\mathcal{A}}x_a\textnormal{exp}(y_a)}
\end{equation*}

When we apply this mirror map to the \ref{equ:OMD} framework we obtain an algorithm named \textit{entropic gradient descent} (\ref{equ:EGD}). For a survey and a more detailed explanation on \ref{equ:EGD} refer to \cite{shalev}. For a pseudo-code description and a schematic representation\footnote{borrowed from \cite[Chapter 2]{HDRmertikopoulos}} of \ref{equ:EGD} see figure \ref{fig:EGDpseudoCodeAndScheme}.

\begin{equation}
    \tag{EGD}
    x_{a,t+1} = \frac{x_{a,t}\textnormal{exp}(\gamma_t v_{a,t})}{\sum_{a'\in\mathcal{A}}x_{a',t}\textnormal{exp}(\gamma_t v_{a',t})}
    \label{equ:EGD}
\end{equation}


\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
    \begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{$\gamma_t > 0$, $\mathcal{X} = \Delta(\mathcal{A})$}
    \For{$t = 1,2,\dots,$T} {
    incur loss $l_t(x_t)$ \;
    receive feedback $v_t \gets -\nabla l_t(x_t)$ \;
    update $x_{t+1} = \frac{(x_{a,t}\textnormal{exp}(\gamma_t v_{a,t}))_{a\in\mathcal{A}}}{\sum_{a'\in\mathcal{A}}x_{a',t}\textnormal{exp}(\gamma_t v_{a',t})}$ \;
    }
    \end{algorithm}
    %\caption{pseudo-code for \ref{equ:POGD}}
    %\label{fig:POGDpseudocode}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{logos/EGDscheme.png}
  %\captionof{figure}{schematic representation of \ref{equ:POGD}}
  %\label{fig:POGDscheme}
\end{subfigure}
\caption{pseudo-code and schematic representation of \ref{equ:EGD}}
\label{fig:EGDpseudoCodeAndScheme}
\end{figure}

To be precise, the figure shows the \textit{lazy} variant \ref{equ:EGD}. It results from the \ref{equ:FTLL} framework with entropic regularization. It is \textit{lazy} because it aggregates the gradients' steps \textit{lazily}, i.e. the gradient step is not performed on the projection but on the previous gradient step. Figure \ref{fig:LazyVsEager} illustrates the difference between \textit{lazy} and \textit{eager} gradient descent\footnote{also borrowed from \cite[Chapter 2]{HDRmertikopoulos}}. An important observation we need to make is that the entropic regularizer becomes infinitely \textit{steep} at the boundary of $\mathcal{X}$. So, the effective domain of the entropic regularization function is the relative interior of $\mathcal{X}$, i.e. dom$h$ = ri($\mathcal{X}$). Therefore projections of the mirror map will always return interior points of $\mathcal{X}$, as figure \ref{fig:EGDpseudoCodeAndScheme} suggests. Note that for steep regularizers, lazy and eager algorithms coincide \cite{HDRmertikopoulos}. Therefore the shown sequence of play in figure \ref{fig:EGDpseudoCodeAndScheme} is the same as is in the eager version of \ref{equ:EGD}. For the Euclidean regularizer, on the other hand, projections lead to points on the boundary of $\mathcal{X}$ like shown in figure \ref{fig:POGDpseudoCodeAndScheme}. The Euclidean regularizer is said to be \textit{non-steep}. \\

\begin{figure}[t]
\centering
  \includegraphics[width=0.5\textwidth]{logos/LazyVsEager.png}
\caption{lazy vs. eager gradient descent}
\label{fig:LazyVsEager}
\end{figure}

Comparing the regret bound between \ref{equ:POGD} and \ref{equ:EGD} in the \textit{multi-armed bandit} problem, for example, we find that \cite{HDRmertikopoulos}

\begin{equation*}
    reg_{POGD}(T) \le 2\sqrt{nT} \qquad \textnormal{and} \qquad reg_{EGD}(T) \le \sqrt{2T\textnormal{log}(n)}
\end{equation*}

As a result, even though both \ref{equ:POGD} and \ref{equ:OMD} enjoy the same regret bound of $\mathcal{O}(\sqrt{T})$ the multiplicative constant involved can have an enormous impact on the algorithm's efficiency. Therefore, \ref{equ:OMD} can be of particular interest for problems that operate in high-dimensional spaces. \\

By now, we have familiarized ourselves with the online mirror descent framework and encountered two concrete no-regret algorithms for constrained problems, namely \textit{projected online gradient descent} (\ref{equ:POGD}) and \textit{entropic gradient descent} (\ref{equ:EGD}). We will later employ these two algorithms on simple two-player games and elaborate on their behavior in chapter \ref{chapter:simulations}. Before that, let us define finite games more formally. 


