
\chapter{Finite Games}\label{chapter:finiteGames}

In a mathematical context, games are models of strategic behavior between players that have individual interests. All games have the same basic structure. They consist of players, each player's set of actions, and some payoff that depends on all players' actions. By defining these three elements, we can derive well-defined solution concepts such as Nash equilibria. The fundamental question of this thesis is: If all players choose their actions according to a \textit{no-regret} update policy in a repeated game, will the outcome be some kind of equilibrium state. Before answering that question, let us introduce the concept of games and equilibria more formally.


\section{Notation and Definitions}\label{section:notationAndDefinitionsGames}

Formally, a finite game is $3$-tuple $\Gamma \equiv (\mathcal{N}, {(\mathcal{A}_i})_{i\in\mathcal{N}},{(u_i)}_{i\in\mathcal{N}})$ where each \textit{player} $i \in \mathcal{N} = \{1,\dots,N\}$ chooses a \textit{pure strategy} (or \textit{action}) $a_i$ from a finite set $\mathcal{A}_i$ of \textit{pure strategies}. The fact that there is only a finite number of players and strategies makes it a \textit{finite} game. Let $\mathcal{A} = \prod_{i}\mathcal{A}_i$ be the \textit{action space} of pure strategies. The players' \textit{payoff}, also called \textit{utility} or \textit{reward}, depends on all players' \textit{strategies}. There are no assumptions on the \textit{players' payoff} function of pure strategies $u_i: \mathcal{A} \to \mathbb{R}$. \\

Additionally, we allow the players to mix strategies. So the strategies that players choose are probability vectors. Formally, in a \textit{mixed extension} of $\Gamma$, players can also play \textit{mixed strategies}, i.e. probability distributions $x_i$ drawn from the probability simplex $\mathcal{X}_i = \Delta(\mathcal{A}_i)  = \{x_i: \sum_{a_i\in \mathcal{A}_i}x_{i,a_i} = 1 \land x_i \ge 0\}$. We can interpret $x_{i,a_i}$ as the probability that player $i \in \mathcal{N}$ assigns to action $a_i \in \mathcal{A}_i$. The expected payoff for some player $i$ depends on a \textit{mixed profile} $x = (x_1,\dots,x_N) \in \mathcal{X} = \prod_{i}\mathcal{X}_i$ and can be written as

\begin{equation*}
    u_i(x) = \sum_{a_1\in\mathcal{A}_1}\dots\sum_{a_N\in\mathcal{A}_N} x_{1,a_1} \dots x_{N,a_N}u_i(a_1,\dots,a_N)
\end{equation*}

When we want to highlight that player $i$ selects strategy $x_i$, we write $x = (x_i;x_{-i})$ where $x_{-i} = {(x_j)}_{j\neq i}$ is the ensemble of strategies selected by the other players. Assuming $u_i$ to be continuously differentiable, we can simply write the players' \textit{individual gradients} as their \textit{payoff} vectors. 

\begin{equation*}
    v_i(x) = \nabla_{x_i}u_i(x_i;x_{-i}) = (u_i(a_i;x_{-i}))_{a_i\in\mathcal{A}_i}
\end{equation*}

In other words, the gradient $v_i(x)$ is the payoff $(u_i(a_i;x_{-i}))_{a_i\in\mathcal{A}_i}$ to player $i$ when $i$ selects $a_i$ against the mixed strategy profile of the other players $x_{-i}$. \\

A finite game is said to be \textit{zero-sum} if for all pure action profiles $a \in \mathcal{A}$ the cumulative utilities of all players sum up to zero.  

\begin{equation}\label{equ:zeroSum}
    \sum_{i = 1}^{N} u_i(a)= 0 \qquad \forall a \in \mathcal{A}
\end{equation}

In the literature, we sometimes find \textit{constant sum} games, i.e. the equation above holds for some constant $c \in \mathbb{R}$. However, constant sum games are equivalent to zero-sum games as they can be normalized. By simply subtracting $\frac{c}{N}$ from all utilities, we obtain a zero-sum game without any loss of information. If a game is neither zero nor constant sum, we say it is a \textit{general-sum} game.\\

Let us define the notion of \textit{dominated strategies} \cite{HDRmertikopoulos}. Formally, given a finite game $\Gamma$, we say that $a_i \in \mathcal{A}_i$ is \textit{strictly dominated} by $a'_i \in \mathcal{A}_i$ if the following holds.

\begin{equation}\label{equ:dominatedStrategy}
    u_i(a_i;x_{-i}) < u_i(a'_i;x_{-i}) \quad \forall x_{-i} \in \mathcal{X}_{-i} \equiv \prod_{j \neq i} \mathcal{X}_j
\end{equation}

If the inequality holds strictly only for some, but not for all $x_{-i} \in \mathcal{X}_{-i}$, then $a_i$ is said to be \textit{weakly dominated} by $a'_i$. Of course, when a dominated strategy is removed from the game, other strategies may become dominated in the resulting game. Strategies that are removed at some point in this repetitive process are called \textit{iteratively dominated strategies}. Note that choosing a strictly dominated strategy is obviously irrational to play. \\

Another way to reason about finite games is to consider \textit{pareto dominance}. A strategy profile $a' \in \mathcal{A}$ is said to \textit{pareto dominate} another profile $a \in \mathcal{A}$ if no player gets less utility with $a'$ than with $a$ and at least one player gets better utility with $a'$ than with $a$. Formally, $a'$ \textit{pareto dominates} $a$ if both statements hold.

\begin{enumerate}\centering
    \item $\forall i \in \mathcal{N}: \quad u_i(a') \ge u_i(a)$
    \item $\exists i \in \mathcal{N}: \quad u_i(a') > u_i(a)$
\end{enumerate}

A strategy profile $a \in \mathcal{A}$ is \textit{pareto optimal} if there is no other profile $a' \in \mathcal{A}$ that pareto dominates $a$. \\

Assume the mixed extension of $\Gamma$ is played over and over again. Let us call this a \textit{repeated game} $\Gamma^{\infty}$. Consider $x^t = (x_{1}^{t},\dots,x_{N}^{t})$ as the strategy profile played in round $t$ where each $x_{i}^{t} \in \Delta(\mathcal{A}_i)$ denotes player $i$'s mixed strategy in round $t$. We can apply the concepts from chapter \ref{chapter:noRegretLearning} where each player $i \in \mathcal{N}$ predicts a mixed strategy $x_{i}^{t}$ in each time step $t$ simultaneously. Then the players receive feedback in the form of the players' individual gradients $v_{i}(x^t) = \nabla_{x_{i}^{t}}u_{i}(x^t)$. We say a strategy $a_i \in \mathcal{A}_i$ becomes \textit{extinct} if $x_{i,a_i}^t$ goes to $0$ as $t$ goes to $\infty$. \\

Unfortunately, there is a misleading difference between \textit{game theory} and \textit{optimization} in terms of how the objective is formulated. In optimization, agents aim to \textit{minimize} losses. However, in game theory, the general convention is that players \textit{maximize} utilities. Therefore, we must assume the utility function to be concave instead of convex. The notion of regret as in definition \ref{def:regret} can be reformulated into a game-theoretic context. The \textit{regret} of player $i$ is then defined as

\begin{equation*}
    reg_{i}(T) = \max_{x_i \in \Delta}\sum_{t=1}^{T} u_i(x_i;x_{-i}^{t}) - \sum_{t=1}^{T}u_i(x^t)
\end{equation*}

Also, note that the derived algorithms in chapter \ref{chapter:noRegretLearning} were \textit{descent} algorithms. As we are maximizing utilities now, they become \textit{ascent} algorithms. In particular, we move in the positive direction of the gradient. For that reason, \textit{projected online gradient descent} (\ref{equ:POGD}) will be named \textit{online projected gradient ascent} (OPGA) hereafter. Similarly, \textit{entropic gradient descent} (\ref{equ:EGD}) is now called \textit{entropic gradient ascent} (EGA). \\

Since the probability simplex $\Delta(\mathcal{A}_i)$ is convex and $u_i$ is linear, which is concave, we can apply these algorithms to \textit{repeated games} and hope that the sequence of play will converge to some notion of equilibrium. Equilibria concepts are discussed in more detail in section \ref{section:equilibriaConcepts}. But before that, let us introduce a well-known example of a finite game to have better intuition.


\section{An Example for a Two Player Game}\label{section:anExample}

The easiest way to define a game in a two-player setting is to draw its \textit{payoff matrix}. For a better understanding, let us consider an example. The zero-sum game Rock Paper Scissors, for instance, has the payoff matrix shown in table \ref{tab:payoffRPSFromDefinition}. \\

\begin{table}\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Rock$}  & \multicolumn{1}{c}{$Paper$}  & \multicolumn{1}{c}{$Scissors$} \\\cline{3-5}
            & $Rock$ & $0,0$ & $-1,1$ & $1,-1$ \\ \cline{3-5}
            & $Paper$ & $1,-1$ & $0,0$ & $-1,1$ \\\cline{3-5}
            & $Scissors$ & $-1,1$ & $1,-1$ & $0,0$ \\\cline{3-5}
\end{tabular}\caption{\label{tab:payoffRPSFromDefinition}payoff matrix Rock Paper Scissors}
\end{table}

Obviously $\mathcal{N} = \{1,2\}$. Following the convention, we will call payer $1$ the \textit{row player} and player $2$ the \textit{column player}, respectively. Both have the same action space $\mathcal{A}_1 = \mathcal{A}_2 = \{\textit{Rock,Paper,Scissor}\} \equiv \{\textit{R,P,S\}}$. Let us abbreviate the actions due to readability. The utilities for pure strategies are given in the payoff matrix, where the first entries refer to the row player and the second entries to the column player. The game is zero-sum because the players' utility sums up to zero for all pure strategy profiles, just like in equation \ref{equ:zeroSum}. So for example, when the row player selects \textit{Paper} while the column player chooses \textit{Rock}, then the row player wins, and the column player loses. In formulas, for $a = (\textit{P,R}) \in \mathcal{A}_1 \times \mathcal{A}_2$ we have

\begin{equation*}
    u_1(a) = 1 \qquad \text{and} \qquad u_2(a) = -1
\end{equation*}

We can use matrix notation and write the utility for mixed strategies $x = (x_1,x_2)$ as

\begin{equation*}
    u_1(x) = x{_1}^{T}Ax_2 \qquad \text{and} \qquad u_2(x) = x{_1}^{T}Bx_2
\end{equation*}

where A,B are the pure strategy payoffs matrices for each player derived from table \ref{tab:payoffRPSFromDefinition}.

\begin{equation*}
A = \begin{pmatrix}
0 & -1 & 1\\
1 & 0 & -1\\
-1 & 1 & 0
\end{pmatrix}
\qquad \text{and} \qquad
B = \begin{pmatrix}
0 & 1 & -1\\
-1 & 0 & 1\\
1 & -1 & 0
\end{pmatrix}
\end{equation*} 

We can think of \textit{mixed strategies} as probability vectors where players assign weights on actions. So for example, the row player can choose to play \textit{Rock} and \textit{Paper} equally likely and not to play \textit{Scissors}. In formulas, $x_1 = (1/2,1/2,0)$. When the row player chooses to play solely rock, i.e. $x_2 = (1,0,0)$ then the expected payoff for both players would be

\begin{equation*}
    u_1(x) = 1/2  \qquad \text{and} \qquad u_2(x) = -1/2
\end{equation*}

Note that strategies are called \textit{pure strategies}, when a single action is assigned with probability 1 and the others with probability 0. For instance, $x_2$ is a pure strategy. Finally, the individual gradients for both players are simply their payoff vectors.

\begin{equation*}
    v_1(x) = \nabla_{x_1}u_1(x) = Ax_2 \qquad \text{and} \qquad v_2(x) = \nabla_{x_2}u_2(x) = x_{1}^{T}B
\end{equation*}


\section{Equilibria Concepts}\label{section:equilibriaConcepts}

This chapter will study several game-theoretic solution concepts, starting with Nash equilibria. After that, the more general notion of correlated equilibria will be introduced along with an example. Generally speaking, while Nash equilibria are rational and hard to compute, correlated equilibria are less rational but easy to compute.

\subsection{Pure and Mixed Nash Equilibria}\label{subsection:PNEandMNE}

The most widely used solution concept in game theory is that of a \textit{Nash equilibrium} (NE). It is a state where no player can increase their expected payoff by changing their strategy while the other players keep their strategy unchanged. In other words, Nash equilibria are those strategy profiles that give no player the incentive to deviate unilaterally. In finite games with mixed extensions, we can distinguish between mixed and pure Nash equilibria. 

\begin{definition}\label{def:MNE}
A \textit{mixed strategy profile} $x^* \in \mathcal{X}$ is called \textit{mixed Nash equilibrium} (MNE) if
    \[u_i(x_{i}^{*};x_{-i}^{*}) \ge  u_i(x_{i};x_{-i}^{*}) \qquad \forall x_i \in \mathcal{X}_i, i \in \mathcal{N}\]
\end{definition}

For instance, the unique MNE in Rock Paper Scissors from section \ref{section:anExample} would be that each player chooses all actions equally likely, i.e. $x_{i}^{*} = (1/3,1/3,1/3)$ for all $i \in \mathcal{N}$. A special case of MNE is when all players choose pure strategies. Remember, a pure strategy simply means that all player $i \in \mathcal{N}$ select one action $a_i \in \mathcal{A}_i$ with probability 1 and the other actions $a_j \in \mathcal{A}_i \setminus \{a_i\}$ with probability 0. We can then define a \textit{pure Nash equilibrium} as follows.

\begin{definition}\label{def:PNE}
    An action profile $a^* \in \mathcal{A}$ is called pure Nash equilibria (PNE) if
    \[u_i(a_{i}^{*};a_{-i}^{*}) \ge u_i(a_{i};a_{-i}^{*}) \qquad \forall a_i \in \mathcal{A}_i, i \in \mathcal{N}\]
\end{definition}

Cleary, all pure Nash equilibria are also mixed Nash equilibria. All $x_{i}^{*}$ are simply unit vectors, then. Let us denote $PNE(\Gamma)$ the game's set of pure Nash equilibria and $MNE(\Gamma)$ as the game's set of mixed Nash equilibria. Then we have that in general

\begin{equation*}
    PNE(\Gamma) \subset MNE(\Gamma)
\end{equation*}

If a Nash equilibrium is not pure in all actions, i.e. $x_i^* \in$ ri$(\mathcal{X}_i)$ for all player $i \in \mathcal{N}$,  we call it an \textit{interior} or \textit{fully mixed} Nash equilibrium. We can further distinguish between \textit{strict} and \textit{weak Nash equilibria}. A Nash equilibrium $x^*$ is called strict when definition \ref{def:MNE} holds with strict inequality for all $x_i \neq x_{i}^{*}$. In other words, $x^*$ is strict if no player can deviate unilaterally from $x^*$ without \textit{reducing} their payoff or, equivalently, when every player has a unique best response to $x^*$. That implies that strict Nash equilibria are pure strategy profiles $x^* = (a_{1}^{*},\dots,a_{N}^{*})$ \cite{HDRmertikopoulos}. So, interior equilibria cannot be strict.

\begin{definition}\label{def:strictNE}
    A pure strategy profile $x^* = (a_{1}^{*},\dots,a_{N}^{*})$ is called strict Nash equilibrium if
    \[u_i(a_{i}^{*};a_{-i}^{*}) > u_i(a_{i};a_{-i}^{*}) \qquad \forall a_i \in \mathcal{A}_i \setminus \{a_{i}^{*}\} , i \in \mathcal{N}\]
\end{definition}

If a NE is not strict, we will call it \textit{weak}. Whether or not a NE is strict plays an essential role in the convergence behavior of no-regret algorithms. We will also discuss the tractability of computing Nash equilibria. It turns out that it is hard. But more on that later in chapter \ref{chapter:literatureReview}. Before that, less common but computationally tractable solution concepts are introduced, namely \textit{correlated} and \textit{coarse correlated equilibria}.

\subsection{Correlated and Coarse Correlated Equilibria}\label{subsection:CEandCCE}

Consider the following example\footnote{University of Pennsylvania, Prof. Aaron Roth, NETS 412 Algorithmic Game Theory, Spring 2017, Lecture 8}

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $0,0$ & $0,1$ \\\cline{3-4}
  & $Go$ & $1,0$ & $-100,-100$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:payoffIntersectionfromFiniteGames}payoff matrix Intersection game}
\end{table}

Imagine an intersection where the players are drivers that can either \textit{Stop} or \textit{Go}, in short, $\mathcal{A}_i = \{S,G\}$ for both players. The players' goal is to \textit{Go} without a crash. When both play \textit{Go} they crash and when both play \textit{Stop} no one gains any utility. The payoff is set accordingly, as in table \ref{tab:payoffIntersectionfromFiniteGames}. \\

There are two pure Nash equilibria, $(S,G)$ and $(G,S)$. Note that at least one player has payoff $0$ in that case. There is also a mixed Nash equilibrium. Suppose the row player selects the strategy $x_1 = (p,1-p)$. Then the column player must be indifferent between both actions, i.e. 

\begin{equation*}
    0 = p - 100(1-p) \iff 101p = 100 \iff p = 100/101
\end{equation*}

So both players choosing \textit{Stop} with probability $p = 100/101$ and \textit{Go} with probability $1-p = 1/101$  leads to a MNE. In terms of utility, this is even worse as the expected payoff for both players is $0$. Under the MNE, the four possible action profiles have roughly the following probability distribution. 

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $98\%$ & $<1\%$ \\\cline{3-4}
  & $Go$ & $<1\%$ & $\approx 0.01\%$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:probabilityUnderMNE}action profile probability distribution under MNE}
\end{table}

A much better outcome would be the subsequent distribution.

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $0\%$ & $50\%$ \\\cline{3-4}
  & $Go$ & $50\%$ & $0\%$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:probabilityUnderCE}action profile probability distribution under CE}
\end{table}

Both players have expected utility $1/2$ and do not risk a crash. The problem is that there is no MNE that yields this probability distribution. The reason is that Nash equilibria are defined as profiles of mixed strategies that require players to randomize independently, without any
communication. The above distribution, however, requires both to correlate their actions. On actual streets, that correlation device is a traffic light. It suggests both drivers whether to \textit{Stop} or \textit{Go}. Following its advice is a best response for everyone. That concept can be formalized. 

\begin{definition}\label{def:CE}
    A \textit{correlated equilibrium} (CE) is a distribution $\mathcal{D}$ over action profiles $\mathcal{A}$ such that for all player $i \in \mathcal{N}$ and every \textit{action} $a_{i}^{*} \in \mathcal{A}_i$
    
    \[\mathbb{E}_{a \sim \mathcal{D}}[u_i(a)] \ge \mathbb{E}_{a \sim \mathcal{D}}[u_i(a_{i}^{*};a_{-i})|a_i]\]
\end{definition}

In words, a CE is a probability distribution over action profiles such that after a profile $a$ is drawn from this distribution, playing $a_i$ is a best response for player $i$ conditioned on seeing $a_i$, given that all the other players play according to $a$. In the intersection game, for instance, conditioned on seeing \textit{Stop}, playing \textit{Stop} is indeed a best response given that the other player sees \textit{Go}. Likewise, conditioned on seeing \textit{Go}, playing \textit{Go} is indeed a best response given that the other player sees \textit{Stop}. \\

Nash equilibira are also correlated equilibria. The difference is just that the players' actions are drawn from an independent distribution, so being conditioned on $a_i$ provides no additional information to $a_{-i}$. Therefore the set of correlated equilibria $CE(\Gamma)$ strictly contains the set of mixed Nash equilibria $MNE(\Gamma)$ in general. Note that in contrast to Nash equilibria, correlated equilibria can be computed efficiently by solving a linear program \cite{HDRmertikopoulos}. An even larger solution concept is the set of \textit{coarse correlated equilibria}. 

\begin{definition}
    A coarse correlated equilibrium (CCE) is a distribution $\mathcal{D}$ over action profiles $\mathcal{A}$ such that for all player $i \in \mathcal{N}$ and \textit{action} $a_{i}^{*} \in \mathcal{A}_i$

    \[\mathbb{E}_{a \sim \mathcal{D}}[u_i(a)] \ge \mathbb{E}_{a \sim \mathcal{D}}[u_i(a_{i}^{*};a_{-i})]\]
\end{definition}

The difference to CE is that a CCE only requires that following a suggested action $a_i$ when $a$ is drawn from $\mathcal{D}$ is only a best response \textit{before} $a_i$ is seen. One can show that there are instances where $CCE(\Gamma)$ is strictly larger than $CE(\Gamma)$ by constructing a distribution\footnote{University of Pennsylvania, Prof. Aaron Roth, NETS 412 Algorithmic Game Theory, Spring 2017, Lecture 8} that is a CCE but not a CE. That yields the following general equilibrium hierarchy.

\begin{equation*}
    PNE(\Gamma) \subset MNE(\Gamma) \subset CE(\Gamma) \subset CCE(\Gamma)
\end{equation*}

Even though a CCE leads to a higher expected payoff, one can construct examples where a CCE assigns positive probability only to strictly dominated strategies \cite{viossat}. Hence, they fail the most basic assumption that players are rational. Therefore, CCE is a relatively weak solution concept. Even though hard to compute, Nash equilibrium remains the most robust and stable solution concept. To what extent do \textit{no-regret dynamics} converge to these solution concepts? That will be discussed in the next chapter.


