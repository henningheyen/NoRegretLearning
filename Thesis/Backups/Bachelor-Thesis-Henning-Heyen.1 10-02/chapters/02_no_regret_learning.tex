
\chapter{No-Regret Learning}\label{chapter:noRegretLearning}

Regret dynamics are one the best studied algorithmic frameworks for online convex optimization. They not only apply in Game Theory but also in Machine Learning and Information Theory. In this chapter the concept of No-Regret Learning will be introduced similar to \cite[Chapter 2 Online Convex Optimization]{shalev}. Later we will cover two concrete No-Regret algorithms, namely Online Gradient Descent with Lazy Projections and Normalized Exponentiated Gradient. In the simulations of chapter \ref{chapter:simulations} we will then run those two algorithms on simple finite games. Note that throughout chapter \ref{chapter:noRegretLearning} we will minimize losses, which is the convention in optimization. In the chapter \ref{chapter:finiteGames}, on the other hand, we are maximizing utilities, which is the convention in game theory.

\section{Notation}\label{section:notation}

Lets first define some basic concepts that we will need in this chapter. Throughout sets are denoted by upper case letters (e.g $\mathcal{S}$) and scalars by lower case letters (e.g $w$). Vectors are in bold letters (e.g $\textbf{w}$) and the $i$-th element of an vector $\textbf{w}$ is written as $w[i]$. In Online Learning a sequence of play is considered. Therefore, we denote $\textbf{w}_t$ the $t$-th vector in the sequence of $\textbf{w}_1,\textbf{w}_2,\dots,\textbf{w}_T$ where $T$ is the number of iterations. \\
The \textbf{inner product} of two vectors $\textbf{u}$ and $\textbf{w}$ with dimension $d$ is defined as 

\begin{equation*}
    \langle \textbf{u},\textbf{w}\rangle = \sum_{i=1}^{d}u[i]w[i]
\end{equation*}

The \textbf{norm} $l_p$ of a vector $\textbf{w}$ is defined as

\begin{equation*}
    \|\textbf{w}\|_p = (\sum_{i}(|w[i]|^p)^{1/p}
\end{equation*}

In particular the $l_2$ (or Euclidean) norm is then $\|\textbf{w}\|_2 = \sqrt{\langle\textbf{w},\textbf{w}\rangle}$. \\

The \textbf{gradient} of a differentiable function $f$ is denoted by $\nabla f$. A function $f$ is called $L-$\textbf{Lipschitz} over a set $\mathcal{S}$ with respect to some norm $\|\cdot\|$ if for all $\textbf{u},\textbf{w} \in \mathcal{S}$ we have that, 

\begin{equation*}
    |f(\textbf{u}) - f(\textbf{w})| \le L\|\textbf{u} - \textbf{w}\|
\end{equation*}

A function $f:\mathcal{S} \to \mathbb{R}$ is said to be \textbf{convex} if for all $\mathbf{u,w}$ and $\lambda \in [0,1]$ we have that 

\begin{equation*}
    f(\lambda\textbf{u} + (1-\lambda)\textbf{w}) \le \lambda f(\textbf{u}) + (1-\lambda)f(\textbf{w})
\end{equation*}

Analogue to that, a set is called \textbf{convex} if for all $\mathbf{u,w} \in \mathcal{S}$ and $\lambda \in [0,1]$ we have that 
\begin{equation*}
    \lambda\mathbf{u} + (1-\lambda)\textbf{w} \in \mathcal{S} 
\end{equation*}


\section{The notion of Regret}\label{section:theNotionOfRegret}

Convexity is crucial for the design of efficient algorithms. We consider the following online convex optimization framework\cite{shalev}: \\

\begin{figure}[ht]\centering
    \textit{Online Convex Optimization}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \KwInput{A $convex$ set $\mathcal{S}$}
        \For{$t = 1,2,\dots,$T} {
        predict strategy $\mathbf{w_t \in \mathcal{S}}$ \;
        receive a $convex$ loss function $f_t:\mathcal{S} \to \mathbb{R}$ \;
        suffer loss $f_t(\mathbf{w_t})$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}


Now, the \textit{Regret} of an algorithm measures how "sorry" the learner is, to not have followed a fixed competing strategy in hindsight. It is the difference between the cumulative loss of the sequence of play and the cumulative loss of some fixed strategy. Therefore, the \textit{Regret}, with respect to some competing strategy $\textbf{u} \in \mathcal{S}$, is defined as

\begin{equation}\label{noRegretEquation}
    Regret_T(\textbf{u}) = \sum_{t=1}^{T}f_t(\textbf{w}_t) - \sum_{t=1}^{T}f_t(\textbf{u})     
\end{equation}

Clearly, we want to minimize the \textit{Regret} with respect to the best fixed strategy in hindsight. So relative to the set of competing strategies $\mathcal{S}$ the \textit{Regret} of an online algorithm is defined as 

\begin{equation}
    Regret_T(\mathcal{S}) = \max_{\textbf{u} \in \mathcal{S}} Regret_T(\textbf{u})
\end{equation}

The learner's goal is to have the lowest \textit{Regret} possible. In fact, we say an algorithm exhibits \textit{No-Regret} if the difference between the cumulative loss of the sequence of play and the cumulative loss of the best fixed strategy tends to zero as $T$, the number of iterations, goes to infinity. Or equivalently, $Regret_T(\mathcal{S})$ grows sublinearly with $T$, i.e

\begin{equation*}
    \text{\textit{No-Regret}} \Leftrightarrow Regret_T(\mathcal{S}) \in o(T)
\end{equation*}


\section{Follow The Leader}\label{section:followTheLeader}

A natural learning rule is to use the one strategy, which had minimal loss on all past rounds. In formulas the update looks like this.

\begin{center}
    \textit{Follow the Leader (FTL)}  
\end{center}
\begin{equation*}
    \textbf{w}_t = \argmin_{\textbf{w} \in \mathcal{S}}\sum_{i=1}^{t-1}f_i(\textbf{w}) \qquad \forall t  
\end{equation*} \\

In the first round we can play any feasible strategy and ties can be broken arbitrarily. While this approach yields \textit{No-Regret} for some loss functions \cite[Cor. 2.2]{shalev}, FTL does not guarantee \textit{No-Regret} in all cases. Consider the following counterexample as in \cite[Ex. 2.2]{shalev}. \\

Let $S = [-1,1] \subset \mathbb{R}$ and consider the sequence of linear functions such that $f_t(w) = z_tw$ where

\begin{equation*}
    z_t = \begin{cases}
    -0.5 &\text{if $t = 1$}\\
    1 &\text{if $t$ is even}\\
    -1 &\text{if $t > 1 \land t$ is odd}
    \end{cases}
\end{equation*} \\

Apart from $t = 1$, where $z_t$ could actually be set arbitrarily in $[-1,1]$, the predictions of FTL will lead to $w_t = 1$ when $t$ is even and $w_t = -1$ when $t$ is odd. Therefore the cumulative loss of FTL will be $T$, while the cumulative loss of the fixed solution $u = 0 \in \mathcal{S}$ is $0$. Hence, the \textit{Regret} of FTL is $T$, which is obviously not sublinearly with $T$. \\

In some sense, FTL seems to be "unstable". The predictions shift drastically from round to round. One way to stablize FTL is to add a so called regularization term. It makes sure that the prediction in the upcoming round is not too "far" off from the current one. 


\section{Follow the Regularized Leader}

We can modify FTL by adding a regularization term. It stabilizes the solution. Let $R:S \to \mathbb{R}$ be a regularization function, then we can define Follow the Regularized Leader as follows

\begin{center}
    \textit{Follow the Regularized Leader (FoReL)}    
\end{center}
\begin{equation}\label{FoReL}
    \textbf{w}_t = \argmin_{\textbf{w} \in \mathcal{S}}\sum_{i=1}^{t-1}f_i(\textbf{w}) + R(\textbf{w}) \qquad \forall t
\end{equation} \\

In the upcoming subsections we will introduce two types of regularizations, the Euclidean and the entropic. Euclidean regularization will lead to what is known as Online Gradient Descent, whereas the entropic regularization provides an instance of Online Mirror Descent. Obviously, different regularization terms give different \textit{Regret} bounds. We will discuss them later as well. 


\subsection{Online Gradient Descent}\label{subsection:onlineGradientAscent}

Consider the Online Linear Optimization problem where $f_t(\textbf{w}) = \langle \textbf{w},\textbf{z}_t \rangle$ and let $S = \mathbb{R}^d$. Applying Euclidean (or $l_2$) regularizer to FoReL will lead to Online Gradient Descent. For some positive step size $\eta$ the Euclidean regularization term is defined as

\begin{equation}\label{euclideanReg}
    R(\textbf{w}) = \frac{1}{2\eta}\|\textbf{w}\|_2^2
\end{equation}

Rearranging the equation in \ref{FoReL} with Euclidean regularization yields the Online Gradient Descent algorithm as follows

\begin{figure}[H]\centering
    \textit{Online Gradient Descent (OGD)}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: }  step size $\eta > 0$ \;
        \textbf{initialize: } $\textbf{w}_1 = \textbf{0}$ \;
        \For{$t = 1,2,\dots,$T} {
        update $\textbf{w}_{t+1} = \textbf{w}_t - \eta\nabla f_t(\textbf{w}_t)$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}

Intuitively, the OGD algorithm moves $\textbf{w}_t$ in the direction of the minimum of $f_t$, but not too much because it wants to remember the effect of $f_1,\dots f_{t-1}$. \\
This way, OGD is a \textit{No-Regret} algorithm. Assume each $f_t$ is $L_t$-Lipschitz with respect to $\|\cdot\|_2$ and let $L$ be such that $\frac{1}{T}\sum_{t=1}^{T}{L_t}^2\le L^2$. Then, for $ \mathcal{S} = \{\textbf{w}: \|\textbf{w}\|_2 \le B\}$ and $\eta = \frac{B}{L\sqrt{2T}}$ we have that \cite[Cor. 2.7]{shalev}

\begin{equation}\label{EuclideanNoRegret}
    Regret_T(\mathcal{S}) \le BL\sqrt{2T} \in o(T)
\end{equation} \\

The problem with OGD is that it does not guarantee that the predictions $\textbf{w}_t$ will always be in the probability simplex, the positive vectors whose elements sum to $1$. Therefore we cannot apply OGD in a Finite Game, where strategies are considered as probability vectors as we will see in Chapter \ref{chapter:finiteGames}. We somehow we have to \textit{project} the predictions back to the probability simplex in this case. In the next section we will discuss two ways to do so, namely Online Gradient Descent with Lazy Projections and Normalized Exponentiated Gradient. 


\subsection{Online Mirror Descent}\label{subsection:onlineMirrorAscent}

The idea of Online Mirror Descent is to find a link function that \textit{mirrors} the gradient step back to the feasible set. Lets define a link function $g: \mathbb{R}^d \to \mathcal{S}$ as 

\begin{equation}\label{link}
    g(\boldsymbol{\theta}) = \argmax_{\textbf{w}} \langle\textbf{w},\boldsymbol{\theta}\rangle - R(\textbf{w})
\end{equation}

where $R(\textbf{w})$ is the regularization term. We can rewrite FoReL from \ref{FoReL} based on the recursive update rule \cite{shalev}: \\

\begin{center}
    \begin{tabular}{ll}
    \textbf{1.} & $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \nabla f(\textbf{w}_t)$\\
    \textbf{2.} & $\textbf{w}_{t+1} = g(\boldsymbol{\theta}_{t+1})$\\
    \end{tabular} 
\end{center}

This yields the Online Mirror Descent Framework. 

\begin{figure}[H]\centering
    \textit{Online Mirror Descent (OMD)}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: } a link function $g: \mathbb{R}^d \to \mathcal{S}$ \;
        \textbf{initialize: } $\boldsymbol{\theta}_1 = \textbf{0}$ \;
        \For{$t = 1,2,\dots,$T} {
        predict strategy $\textbf{w}_t = g(\boldsymbol{\theta}_t)$ \;
        update $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \nabla f(\textbf{w}_t)$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}

Setting $\mathcal{S}= \mathbb{R}^d$ and $g(\boldsymbol{\theta}) = \eta\boldsymbol{\theta}$ we simply obtain Online Gradient Descent. So OMD is a generalization of OGD. \\

As we focus on Finite Games and mixed strategies later, we need the predictions to be probability vectors. Lets constrain $\mathcal{S}$ to be the probability simplex from now on and denote it by $\Delta$. It is defined as 

\begin{equation*}
    \Delta = \{\textbf{w}: \|\textbf{w}\|_1 = 1 \land \textbf{w} \ge \textbf{0}\}
\end{equation*}

In the following we will introduce two common link functions that mirror the predictions back to $\Delta$. 

\subsubsection{Online Gradient Descent with Lazy Projections}\label{subsubsection:OnlineGradientDescentWithLazyProjections}

Analogue to Online Gradient Descent we update the predictions of the algorithm at each time step moving in the negative direction of the gradient of the loss function. But this time we project the predictions back to the probability simplex using Euclidean projection. The link function $g(\boldsymbol{\theta})$ simply returns the closest point in $\Delta$ relative to the gradient step $\eta\boldsymbol{\theta}$. Formally, 

\begin{equation*}
    g(\boldsymbol{\theta}) = \argmin_{\textbf{w} \in \Delta} \|\textbf{w} - \eta\boldsymbol{\theta}\|_2
\end{equation*} \\

By applying this link function to the OMD framework we obtain an algorithm known as Online Gradient Descent with Lazy Projections:

\begin{figure}[H]\centering
    \textit{Online Gradient Descent \\ with Lazy Projections (OGDLP)} 
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: } step size $\eta > 0$ \;
        \textbf{initialize: } $\boldsymbol{\theta}_1 = \textbf{0}$ \;
        \For{$t = 1,2,\dots,$T} {
        predict strategy $\textbf{w}_t = \argmin_{\textbf{w} \in \Delta} \|\textbf{w} - \eta\boldsymbol{\theta}_t\|_2$ \;
        update $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \nabla f(\textbf{w}_t)$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}

As shown in \cite[Cor. 2.17]{shalev} the Euclidean projection has no impact on the \textit{Regret}. Since OGDLP is induced by Euclidean regularization it enjoys the same \textit{Regret} bound as in \ref{EuclideanNoRegret} where $B = 1$ as we consider the feasible set to be the probability simplex. Consequently, OGDLP is considered as a \textit{No-Regret} algorithm. \\

A possible disadvantage of the Online Gradient Descent approach so far is that it requires solving an optimization problem at each online round. The next algorithm yields the \textit{No-Regret} property while having a much simpler update rule.


\subsubsection{Normalized Exponentiated Gradient}\label{subsubsection:normalizedExponentiatedGradient}

Another suitable regularizer for simplex constrained optimization problems is the entropic regularization. It is defined as 

\begin{equation}\label{entropicReg}
    R(\textbf{w}) = \frac{1}{\eta} \sum_{i}w[i]log(w[i])
\end{equation}

where $log(\cdot)$ is the natural logarithm. Plugging in the entropic regularization in the link function from \ref{link} and rearranging yields the following mirror whose $i$-th component is the function \cite{shalev}

\begin{equation*}
    g_i(\boldsymbol{\theta}) = \frac{e^{\eta\theta[i]}}{\sum_{j}e^{\eta\theta[j]}}
\end{equation*} \\

According to the OMD framework we obtain the following simple update rule \cite{shalev}

\begin{equation*}
    \textbf{w}_{t+1}[i] = \frac{w_t[i]e^{-\eta\nabla f_t[i]}}{\sum_{j}w_t[j]e^{-\eta\nabla f_t[j]}}
\end{equation*} \\

The derived algorithm is called Normalized Exponentiated Gradient. Note that we cannot initialize $\textbf{w}_1 = \textbf{0}$ as the entropic regularizer is not defined then. We need to choose some interior point of $\Delta$. \\

\begin{figure}[H]\centering
    \textit{Normalized Exponentiated Gradient (NEG)}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: } step size $\eta > 0$ \;
        \textbf{initialize: } $\{\textbf{w}: \|\textbf{w}\|_1 = 1 \land \textbf{w} > \textbf{0}\}$ \; 
        \For{$t = 1,2,\dots,$T} { 
        update $\textbf{w}_{t+1}[i] = \scaleto{\frac{w_t[i]e^{-\eta\nabla f_t[i]}}{\sum_{j}w_t[j]e^{-\eta\nabla f_t[j]}}}{27pt} \quad \forall i$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}

Just like OGD, NEG is a \textit{No-Regret} algorithm. Assume each $f_t$ is $L_t$-Lipschitz with respect to $\|\cdot\|_1$ and let $L$ be such that $\frac{1}{T}\sum_{t=1}^{T}{L_t}^2\le L^2$. Let $S = \Delta$ and $\eta = \frac{\sqrt{log(d)}}{L\sqrt{2T}}$. Running FoReL with entropic regularization we have that \cite[Cor. 2.14]{shalev}

\begin{equation}\label{EntropicNoRegret}
    Regret_T(\mathcal{S}) \le L\sqrt{2log(d)T} \in o(T)
\end{equation} \\

To sum up, we introduced two \textit{No-Regret} Algorithms that can be applied for simplex constrained convex optimization problems. First, Online Gradient Descent with Lazy Projections (OGDLP) which was based on Euclidean regularization and second, Normalized Exponentiated Gradient (NEG) induced by entropic regularization. We will run both algorithms in simple finite games later in chapter \ref{chapter:simulations}. Before that, lets define finite games and their equilibira concepts. 

