
\chapter{No Regret Learning}\label{chapter:noRegretLearning}

Regret dynamics are one the best studied algorithmic frameworks for online convex optimization. They not only apply in Game Theory but also in Machine Learning and Information Theory. In this chapter the concept of No Regret Learning will be introduced similar to \cite[Chapter 2 Online Convex Optimization]{shalev}. Later two concrete no-regret algorithms, namely Normalized Exponentiated Gradient and  Online Gradient Descent with Lazy Projections, will be covered. Both are then used in the chapter Simulations.

\section{The notion of Regret}\label{section:theNotionOfRegret}

\begin{itemize}
    \item define Regret as in \cite{shalev} p.119/120
\end{itemize}

\section{Follow The Leader}\label{section:followTheLeader}

\begin{itemize}
    \item Introduce FTL as in \cite{shalev} 2.2 Follow-the-Leader p.124
    \item Show that FTL does not guarantee no regret. Counter example in \cite{shalev} p.127 (Ex. 2.2 Failure of FTL) or in https://parameterfree.com/2019/09/11/online-gradient-descent/ 
\end{itemize}

\section{Follow the Regularized Leader}

\begin{itemize}
    \item Regularization stabilizes the predictions. as described in 
    \cite{shalev} p. 129
    \item Shortly introduce two ways of regularization (Euclidean and Entropic)
\end{itemize}

\subsection{Online Gradient Descent}\label{subsection:onlineGradientAscent}

\begin{itemize}
    \item algorithm as in \cite{shalev} p.132
    \item uses Euclidean Regularization
    \item OGD yields no regret bound as in Cor 2.7 p. 134
    \item \cite{shalev} p. 141 A possible disadvantage of the FoReL approach so far is that it requires solving an optimization problem at each online round. OMD yields same regret bound but simpler update rule
    \item cannot apply OGD framwork as it does not guarantee that the predictions
    will always be in the probability simplex which is the feasible set for 
    finite games with mixed extensions. Will later introduce a Regularizer
    (Entropic) that guarantees that and also introduce OGD with Lazy Projections.
\end{itemize}
Algorithm is described in https://parameterfree.com/2019/09/11/online-gradient-descent/ 

\subsection{Online Mirror Descent}\label{subsection:onlineMirrorAscent}

\begin{itemize}
    \item \cite{mertikopoulos} p.468: Nice explanation how OMA works on a high level
    \item \cite{mertikopoulos} p.477: Nice Figure of OMA 
    \item in "ComputingBNE, p.3" short explanation of OMA 
    \item introduce algorithm as in \cite{shalev} p. 141 2.6 Online Mirror Descent
    \item need to restore injectivity by a mirror on the convex set, the simplex in games, see \cite{flokas} p.7
\end{itemize}

\subsubsection{Normalized Exponentiated Gradient}\label{subsubsection:normalizedExponentiatedGradient}

\begin{itemize}
    \item introduce strong convexity to give intuition why OMD makes more sense
    than OGD. as in \cite{shalev} p.134 Since the Entropic Regularizer is 1/B
    strongy convex w.r.t to L1 norm (Ex 2.5)
    whereas the Euclidean Regularizer is 1-strongy convex w.r.t
    to L2 norm (Ex 2.4). We obtain better regret bounds using OMA
    (BLsqrt(2log(d)T)) than with OMD (BLsqrt(2T)) (Cor. 2.14/2.15)
    \item Derive the Normalized Exponentiated Gradient algorithm \cite{shalev}
    p.143.
    \item Entropic regularization also mentionend in \cite{flokas} Ex. 2.1 or \cite{mertikopoulos} Ex. 3.2
    
    \item Online Mirror Ascent (OMA) is based on the following recursive update rules (see ComputingBNE p.10)
\end{itemize}

\subsubsection{Online Gradient Descent with Lazy Projections}\label{subsubsection:OnlineGradientDescentWithLazyProjections}

\begin{itemize}
    \item introduce the algorithm as in \cite{shalev} p. 144
    \item informally: It consists in updating the prediction of the algorithm at
    each time step moving in the negative direction of the gradient of the loss
    received and projecting back onto the feasible set
    (https://parameterfree.com/2019/09/11/online-gradient-descent/).
    \item same regret bound as in Cor. 2.13
    \item Update rule for Normalized Exponentiated Gradient algorithm is much
    simpler than for FoReL \cite{shalev} p.141
    \item L2 regularization also mentionend in \cite{flokas} Ex. 2.2 or \cite{mertikopoulos} Ex. 3.1
\end{itemize}




