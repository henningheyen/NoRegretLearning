
\chapter{Finite Games}\label{chapter:finiteGames}

\begin{itemize}
    \item high level description of an finite game
    \item structure of this chapter
    \item tell that we first define the more general version of a \textit{contiuous} game an then mixed extensions
    
\end{itemize}

\begin{comment}
    We are thus led to the following fundamental question: if all players of a repeated game employ a no-regret updating policy, do their actions converge to a Nash equilibrium of the underlying game? In general, the answer to this question is a resounding “no”. Even in simple, finite games, no-regret learning may cycle [27] and its limit set may contain highly non- rationalizable strategies that assign positive weight only to strictly dominated strategies [48]. As such, our aim in this paper is twofold \cite{mertikopoulos} p.466
\end{comment}

\section{Notation and Definitions}\label{section:notationAndDefinitionsGames}

Formally, a continuous game is a tuple $\mathcal{G} \equiv (\mathcal{N}, {(\mathcal{X}_i})_{i\in\mathcal{N}},{(u_i)}_{i\in\mathcal{N}})$ where each \textit{player} $i \in \mathcal{N} = \{1,\dots,N\}$ chooses an \textit{action} $x_i$ from a compact convex subset $\mathcal{X}_i$ of a finite-dimnesional normed space $\mathcal{V}_i$. Let $\mathcal{X} = \prod_{i}\mathcal{X}_i$ be the \textit{action space}. The \textit{players' payoff}, also called \textit{utility} or \textit{reward}, depends on all \textit{players' actions}. An \textit{action profile} is denoted by $x = (x_1,\dots,x_N) \in \mathcal{X}$. When we want to highlight that \textit{player} $i$ selects \textit{action} $x_i$ we write $x = (x_i,x_{-i})$ where $x_{-i} = {(x_j)}_{j\neq i}$ is the ensemble of \textit{actions} selected by the other \textit{players}. The \textit{players' payoff} is determined by a \textit{payoff }function $u_i : \mathcal{X} \to \mathbb{R} $. Assuming $u_i$ to be continuously differentiable we write 

\begin{equation*}
    v_i(x) = \nabla_{x_i}u_i(x_i,x_{-i})
\end{equation*}

for the \textit{individual gradient} of $u_i$ at $x$. As as special case we consider \textit{payoff} functions that are \textit{individually concave} in a sense that 

\begin{equation}\label{concaveGame}
    u_i(x_i,x_{-i}) \textit{ is concave in $x_i$ for all $x_{-i} \in \prod_{j\neq i}\mathcal{X}_j,i \in \mathcal{N}$}
\end{equation}

When this is the case we say that the game itself is \textit{concave}. A well known version of a such a game is a \textit{finite game with mixed extensions}. \\

In finite games with mixed extensions we allow players to "mix" the actions they want to play. So the strategies that players choose are probability vector. Formally in finite game $\Gamma \equiv (\mathcal{N}, {(\mathcal{A}_i})_{i\in\mathcal{N}},{(u_i)}_{i\in\mathcal{N}})$ each \textit{player} $i \in \mathcal{N}$ chooses an \textit{action} $a_i$ from a finite set $\mathcal{A}_i$ of \textit{pure strategies}. Let $\mathcal{A} = \prod_{i}\mathcal{A}_i$ be the \textit{action space} of \textit{pure strategies}. There are no assumptions on the \textit{players' payoff} function $u_i: \mathcal{A} \to \mathbb{R}$. \textit{Players} can also play \textit{mixed strategies}, i.e probability distributions $x_i$ drawn from the probability simplex $\mathcal{X}_i = \Delta(\mathcal{A}_i)$. In that case the expected \textit{payoff} of some \textit{player} $i$ depends on a \textit{mixed profile} $x = (x_1,\dots,x_N) \in \mathcal{X} = \prod_{i}\mathcal{X}_i$ and can be written as 

\begin{equation*}
    u_i(x) = \sum_{a_1\in\mathcal{A}_1}\dots\sum_{a_N\in\mathcal{A}_N} x_{1,a_1} \dots x_{N,a_N}u_i(a_1,\dots,a_N)
\end{equation*}

Then the \textit{players' individual gradients} are simply their \textit{payoff} vectors 

\begin{equation*}
    v_i(x) = \nabla_{x_i}u_i(x) = (u_i(a_i,x_{-i}))_{a_i\in\mathcal{A}_i}
\end{equation*}

The resulting continuous game is called \textit{mixed extension} of $\Gamma$. Because $\mathcal{X}_i = \Delta(\mathcal{A}_i)$ is convex and $u_i$ is linear in $x_i$, it is \textit{concave} in the sense of equation \ref{concaveGame}. \\

Lets assume the \textit{mixed extension} of $\Gamma$ is played over and over again and call this a \textit{repeated game} $\Gamma^{\infty}$. Consider $x^t = (x_{1}^{t},\dots,x_{N}^{t})$ as the \textit{strategy profile} that was played in round $t$ where each $x_{i}^{t} \in \Delta(\mathcal{A}_i)$ denotes \textit{player} $i$'s \textit{mixed strategy} in round $t$. We can apply the concepts from chapter \ref{chapter:noRegretLearning} where each \textit{player} $i \in \mathcal{N}$ predicts a \textit{mixed strategy} $x_{i}^{t}$ in each iteration step $t$ simultaneously. Then the \textit{players} receive feedback in form of the  \textit{players' individual gradients} $v_{i}(x^t) = \nabla_{x_{i}^{t}}u_{i}(x^t)$. \\

The notion of regret as in definition \ref{def:regret} can be reformulated into a game theoretic context. The \textit{regret} of \textit{player} $i$ is then defined as

\begin{equation*}
    regret_{i}^{T}(\Delta) = \max_{x_i \in \Delta}\sum_{t=1}^{T} u_i(x_i,x_{-i}^{t}) - \sum_{t=1}^{T}u_i(x^t)
\end{equation*}

As the \textit{players'} goal is to maximize their \textit{utility} we turn the convex loss function from chapter \ref{chapter:noRegretLearning} into a concave \textit{utility} function which is $u_i$. Thereby the "Descent" algorithms become "Ascent" algorithms. In particular we move in the positive direction of the gradient. For that reason \textit{Online Gradient Descent with Lazy Projections} (OGDLP) will be named \textit{Online Gradient Ascent with Lazy Projections} (OGALP) hereafter. We continue to say \textit{Normalized Exponentiated Gradient} (NEG) but be aware of this slight reformulation of the algorithms. \\

Since the probability simplex $\Delta(\mathcal{A}_i)$ is convex and $u_i$ is linear, which is concave, we can apply these algorithms to \textit{repeated games} and hope that the sequence of play will converge to some notion of equilibrium. Equilibria concepts are discussed in more detail in section \ref{section:equilibriaConcepts}. Before that, lets introduce a well known example of a finite game with mixed extensions in order to have a better intuition. 


\section{An Example}\label{section:anExample}

Note that for two player games the easiest way to define a finite game is by drawing its \textit{payoff matrix}. For a better understanding lets consider an example. The game Rock Paper Scissors, for instance, has the following \textit{payoff matrix}.\\

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Rock$}  & \multicolumn{1}{c}{$Paper$}  & \multicolumn{1}{c}{$Scissors$} \\\cline{3-5}
            & $Rock$ & $0,0$ & $-1,1$ & $1,-1$ \\ \cline{3-5}
            & $Paper$ & $1,-1$ & $0,0$ & $-1,1$ \\\cline{3-5}
            & $Scissors$ & $-1,1$ & $1,-1$ & $0,0$ \\\cline{3-5}
\end{tabular}\caption{\label{tab:payoffRPSFromDefinition}\textit{payoff matrix} Rock Paper Scissors}
\end{table}

Obviously $\mathcal{N} = \{1,2\}$. Following the convention, we will call \textit{payer $1$} the \textit{row player} and \textit{player $2$} the \textit{column player} respectively. Both have the same \textit{action space} $\mathcal{A}_1 = \mathcal{A}_2 = \{\textit{Rock,Paper,Scissor}\} \equiv \{\textit{R,P,S\}}$. Lets abbreviate the actions due to readability. The utilities for \textit{pure strategies} are given in the \textit{payoff matrix}, where the first entries refer to the \textit{row player} and the second entries to the \textit{column player}. So for example, when the \textit{row player} selects \textit{Paper} while the \textit{column player} chooses \textit{Rock} then the \textit{row player} wins and the \textit{column player} looses. In formulas

\begin{equation*}
    u_1(\textit{P,R}) = 1 \qquad \text{and} \qquad u_2(\textit{P,R}) = -1
\end{equation*}

We can use matrix notation and write the \textit{utility} for \textit{mixed strategies} $x = (x_1,x_2)$ as

\begin{equation*}
    u_1(x) = x{_1}^{T}Ax_2 \qquad \text{and} \qquad u_2(x) = x{_1}^{T}Bx_2
\end{equation*}

where A,B are the \textit{pure strategy payoffs matrices} for each player obtained by table \ref{tab:payoffRPSFromDefinition}.

\begin{equation*}
A = \begin{pmatrix}
0 & -1 & 1\\
1 & 0 & -1\\
-1 & 1 & 0
\end{pmatrix}
\qquad \text{and} \qquad
B = \begin{pmatrix}
0 & 1 & -1\\
-1 & 0 & 1\\
1 & -1 & 0
\end{pmatrix}
\end{equation*}

We can think of \textit{mixed strategies} as probability vectors where \textit{players} assign weights on actions. So for example the \textit{row} player can choose to play \textit{Rock} and \textit{Paper} equally likely and not to play \textit{Scissors}. In formulas, $x_1 = (1/2,1/2,0)$. When the \textit{row player} chooses to play solely rock, i.e $x_2 = (1,0,0)$ then the expected \textit{payoff} for both \textit{players} would be

\begin{equation*}
    u_1(x) = 1/2  \qquad \text{and} \qquad u_2(x) = -1/2
\end{equation*}

Note that strategies where a single \textit{action} is assigned with probability 1 and the others with probability 0 are called \textit{pure strategies} and \textit{fully mixed strategies} otherwise. Both are are feasible \textit{mixed strategies} though. \\ 

Finally the \textit{individual gradients} for both \textit{players} are simply their \textit{payoff} vectors.

\begin{equation*}
    v_1(x) = \nabla_{x_1}u_1(x) = Ax_2 \qquad \text{and} \qquad v_2(x) = \nabla_{x_2}u_2(x) = x_{1}^{T}B
\end{equation*}


\section{Equilibria Concepts}\label{section:equilibriaConcepts}


\subsection{Pure and Mixed Nash Equilibria}\label{subsection:PNEandMNE}
The most widely used solution concept in game theory is that of a \textit{Nash equilibrium}. It is a state, where no player can increase their own expected \textit{payoff} by changing their \textit{strategy} while the other \textit{players} keep their \textit{strategy} unchanged. In other words, \textit{Nash equilibria} are those \textit{strategy profiles} that give no \textit{player} the incentive to unilaterally deviate. In finite games with mixed extentions we can we can distinguish between \textit{mixed} and \textit{pure Nash equilibria}. 

\begin{definition}\label{def:MNE}
A \textit{mixed strategy profile} $x^* \in \mathcal{X}$ is called \textit{mixed Nash equilibrium} (MNE) if
    \[u_i(x_{i}^{*},x_{-i}^{*}) \le  u_i(x_{i},x_{-i}^{*}) \qquad \forall \quad x_i \in \mathcal{X}_i, i \in \mathcal{N}\]
\end{definition}

For instance, the unique MNE in Rock Paper Scissors from section \ref{section:anExample} would be that each player chooses all actions equally likely, i.e $x_{i}^{*} = (1/3,1/3,1/3) \quad \forall i \in \mathcal{N}$. A special case of MNE is when all \textit{players} choose \textit{pure strategies}. A \textit{pure strategy} simply means that all \textit{player} $i \in \mathcal{N}$ select one \textit{action} $a_i \in \mathcal{A}_i$ with probability 1 and the other \textit{actions} $a_j \in \mathcal{A}_i \setminus \{a_i\}$ with probability 0. We can then define a \textit{pure Nash equilibrium} as follows.

\begin{definition}\label{def:PNE}
    An \textit{action profile} $a^* \in \mathcal{A}$ is called \textit{pure Nash equilibrium} (PNE) if
    \[u_i(a_{i}^{*},a_{-i}^{*}) \le u_i(a_{i},a_{-i}^{*}) \qquad \forall \quad a_i \in \mathcal{A}_i, i \in \mathcal{N}\]
\end{definition}

Cleary, all \textit{pure Nash equilibria} are also \textit{mixed Nash equilibria}. All $x_{i}^{*}$ are simply unit vectors then. When we think of PNE and MNE as sets then we have 

\begin{equation*}
    PNE \subset MNE
\end{equation*}

We can further distinguish between \textit{strict} and \textit{weak Nash equilibria}. A \textit{Nash equilibrium} $x^*$ is called \textit{strict} when definition \ref{def:MNE} holds with strict inequality for all $x_i \neq x_{i}^{*}$. In other words, $x^*$ is \textit{strict} if no player can deviate unilaterally from $x^*$ without \textit{reducing} their payoff or, equivalently, when every player has a unique best response to $x^*$. This implies that \textit{strict Nash equilibria} are \textit{pure strategy} profiles $x^* = (a_{1}^{*},\dots,a_{N}^{*})$ such that

\begin{definition}\label{def:strictNE}
    A \textit{pure strategy} profile $x^* = (a_{1}^{*},\dots,a_{N}^{*})$ is called \textit{strict Nash Equilibrium} if
    \[u_i(a_{i}^{*},a_{-i}^{*}) < u_i(a_{i},a_{-i}^{*}) \qquad \forall \quad a_i \in \mathcal{A}_i \setminus \{a_{i}^{*}\} , i \in \mathcal{N}\]
\end{definition}

If a NE is not \textit{strict} we will call it \textit{weak}. Whether or not a NE is \textit{strict} plays an important role when it comes to the convergence behaviour of \textit{no-regret} algorithms. We will also discuss the tractability of computing \textit{Nash Equilibria}. It turns out that its hard. But more on that later in chapter \ref{chapter:literatureReview}. Before that, a less common but in fact computationally tractable solution concept is introduced, namely \textit{correlated} and \textit{coarse correlated equilibria}.\\

\subsection{Correlated and Coarse Correlated Equilibria}\label{subsection:CEandCCE}

Consider the following example\footnote{University of Pennsylvania, Prof. Aaron Roth, NETS 412 Algorithmic Game Theory, Lecture 8}

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $0,0$ & $0,1$ \\\cline{3-4}
  & $Go$ & $1,0$ & $-100,-100$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:payoffIntersectionfromFiniteGames}payoff matrix Intersection Game}
\end{table}

Imagine an intersection where the players are drivers that can either \textit{Stop} or \textit{Go}. In short, $\mathcal{A}_i = \{S,G\}$ for both players. The players' goal is to \textit{Go} without a crash. When both play \textit{Go} they crash and when both play \textit{Stop} no one gains any utility. The \textit{payoff} is set accordingly as in table \ref{tab:payoffIntersectionfromFiniteGames}. \\

There are two \textit{pure Nash equilibria}, $(S,G)$ and $(G,S)$. Note that at least one player has payoff $0$ in that case. There is also a \textit{mixed Nash equilibrium}. Suppose the row player selects the \textit{strategy} $x_1 = (p,1-p)$. Then the column player must be indifferent between both actions, i.e 

\begin{equation*}
    0 = p - 100(1-p) \iff 101p = 100 \iff p = 100/101
\end{equation*}

So both players choosing \textit{Stop} with probability $p = 100/101$ and \textit{Go} with probability $1-p = 1/101$  leads to a MNE. In terms of utility, this is even worse as the expected payoff for both players is $0$. Under the MNE the four possible action
profiles have roughly the following probabilities. 

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $98\%$ & $<1\%$ \\\cline{3-4}
  & $Go$ & $<1\%$ & $\approx 0.01\%$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:probabilityUnderMNE}action profile probability distribution under MNE}
\end{table}

A much better outcome would the following distribution

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $0\%$ & $50\%$ \\\cline{3-4}
  & $Go$ & $50\%$ & $0\%$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:probabilityUnderCE}action profile probability distribution under CE}
\end{table}

Both player have expected utility $1/2$ and don't have to risk a crash. The problem is that there is no MNE that yields this probability distribution. The reason is that \textit{Nash equilibria} are defined as profiles of mixed strategies, that require players to randomize independently, without any
communication. The above distribution, however, requires both to correlate their actions. On real streets that correlation device is a traffic light. It suggests both driver whether to \textit{Stop} or to \textit{Go}. Following its advice is a best response for everyone. \\

That concept can be formalized. A \textit{correlated equilibrium} (CE) is a distribution $\mathcal{D}$ over action profiles $\mathcal{A}$ such that for all \textit{player} $i$ and \textit{action} $a_{i}^{*}$

\begin{equation*}
    \mathbb{E}_{a \sim \mathcal{D}}[u_i(a)] \ge \mathbb{E}_{a \sim \mathcal{D}}[u_i(a_{i}^{*},a_{-i})|a_i]
\end{equation*}

In words, a CE is a probability distribution over \textit{action profiles} such that after a profile $a$ is drawn from this distribution, playing $a_i$ is a best response for player $i$ conditioned on seeing $a_i$, given that all the other players play according to $a$. In the intersection game, for instance, conditioned on seeing \textit{Stop}, playing \textit{Stop} is indeed a best response given that the other player sees \textit{Go}. Likewise, conditioned on seeing \textit{Go}, playing \textit{Go} is indeed a best response given that the other player sees \textit{Stop}. \\

\textit{Nash equilibira} are also \textit{correlated equilibria}. It's just that the players' actions are drawn from an independent distribution, so being conditioned on $a_i$ provides no additional information to $a_{-i}$. Therefore the set of CE strictly contains the set of MNE. An even larger solution concept is the set of \textit{coarse correlated equilibria} (CCE). A CCE is a distribution $\mathcal{D}$ over \textit{action profiles} $\mathcal{A}$ such that for all player $i$ and \textit{action} $a_{i}^{*}$

\begin{equation*}
    \mathbb{E}_{a \sim \mathcal{D}}[u_i(a)] \ge \mathbb{E}_{a \sim \mathcal{D}}[u_i(a_{i}^{*},a_{-i})]
\end{equation*}

The difference to CE is that CCE only requires that following a suggested \textit{action} $a_i$ when $a$ is drawn from $\mathcal{D}$ is only a best response \textit{before} $a_i$ is seen. One can show that the set of CCE is strictly larger than the set of CE\footnote{University of Pennsylvania, Prof. Aaron Roth, NETS 412 Algorithmic Game Theory, Lecture 8}. That yields the following equilibria hierarchy

\begin{equation*}
    PNE \subset MNE \subset CE \subset CCE
\end{equation*}

In contrast to CE, CCE may contain strictly dominated \textit{pure strategy profiles} with positive probability, so they fail the most basic assumption that players are rational. \cite{viossat}. This means that CCE is a rather weak solution concept. Even though hard to compute, \textit{Nash equilibria} is still the most robust and stable solution concept. To what extent \textit{no-regret dynamics} converge to equilibria will be subject of the next chapter. 


