
\chapter{No-Regret Learning}\label{chapter:noRegretLearning}

Regret dynamics are one the best studied algorithmic frameworks for online convex optimization. They not only apply in Game Theory but also in Machine Learning and Information Theory. In this chapter the concept of no-regret Learning will be introduced similar to \cite[Chapter 2 Online Convex Optimization]{shalev}. Later we will cover two concrete no-regret algorithms, namely Online Gradient Descent with Lazy Projections and Normalized Exponentiated Gradient. In the simulations of chapter \ref{chapter:simulations} we will then run those two algorithms on simple finite games. Note that throughout chapter \ref{chapter:noRegretLearning} we will minimize losses, which is the convention in optimization. In the chapter \ref{chapter:finiteGames}, on the other hand, we are maximizing utilities, which is the convention in game theory.

\section{Notation and Definitions}\label{section:notationAndDefinitionsRegret}

Lets first define some basic concepts that we will need in this chapter. Throughout sets are denoted by upper case letters (e.g $\mathcal{S}$) and scalars by lower case letters (e.g $w$). Vectors are in bold letters (e.g $\boldsymbol{w}$) and the $i$-th element of an vector $\boldsymbol{w}$ is written as $w[i]$. In Online Learning a sequence of play is considered. Therefore, we denote $\boldsymbol{w}_t$ the $t$-th vector in the sequence of $\boldsymbol{w}_1,\boldsymbol{w}_2,\dots,\boldsymbol{w}_T$ where $T$ is the number of iterations. \\
The \textbf{inner product} of two vectors $\boldsymbol{u}$ and $\boldsymbol{w}$ with dimension $d$ is defined as 

\begin{equation*}
    \langle \boldsymbol{u},\boldsymbol{w}\rangle = \sum_{i=1}^{d}u[i]w[i]
\end{equation*}

The \textbf{norm} $l_p$ of a vector $\boldsymbol{w}$ is defined as

\begin{equation*}
    \|\boldsymbol{w}\|_p = (\sum_{i}(|w[i]|^p)^{1/p}
\end{equation*}

In particular the $l_2$ (or Euclidean) norm is then $\|\boldsymbol{w}\|_2 = \sqrt{\langle\boldsymbol{w},\boldsymbol{w}\rangle}$. \\

The \textbf{gradient} of a differentiable function $f: \mathcal{S} \to \mathbb{R}$ is denoted by $\nabla f$. A function $f$ is called $L-$\textbf{Lipschitz} over a set $\mathcal{S}$ with respect to some norm $\|\cdot\|$ if for all $\boldsymbol{u},\boldsymbol{w} \in \mathcal{S}$ we have that, 

\begin{equation*}
    |f(\boldsymbol{u}) - f(\boldsymbol{w})| \le L\|\boldsymbol{u} - \boldsymbol{w}\|
\end{equation*}

A function $f:\mathcal{S} \to \mathbb{R}$ is said to be \textbf{convex} if for all $\mathbf{u,w}$ and $\lambda \in [0,1]$ we have that 

\begin{equation*}
    f(\lambda\boldsymbol{u} + (1-\lambda)\boldsymbol{w}) \le \lambda f(\boldsymbol{u}) + (1-\lambda)f(\boldsymbol{w})
\end{equation*}

Equivalently, a function $f:\mathcal{S} \to \mathbb{R}$ is said to be \textbf{concave} if for all $\mathbf{u,w}$ and $\lambda \in [0,1]$ we have that 

\begin{equation*}
    f(\lambda\boldsymbol{u} + (1-\lambda)\boldsymbol{w}) \ge \lambda f(\boldsymbol{u}) + (1-\lambda)f(\boldsymbol{w})
\end{equation*}

A function $f:\mathcal{S} \to \mathbb{R}$ is $\sigma-$\textbf{strongly-convex} over $\mathcal{S}$ with respect to some norm $\|\cdot\|$ if for any $\boldsymbol{w} \in \mathcal{S}$ and for all $\boldsymbol{u} \in \mathcal{S}$ we have 

\begin{equation*}
    f(\boldsymbol{u}) \ge f(\boldsymbol{w}) + \langle\nabla f(\boldsymbol{w}),\boldsymbol{u} - \boldsymbol{w}\rangle + \frac{\sigma}{2}\|\boldsymbol{u} - \boldsymbol{w}\|
\end{equation*}

A set is called \textbf{convex} if for all $\mathbf{u,w} \in \mathcal{S}$ and $\lambda \in [0,1]$ we have that 
\begin{equation*}
    \lambda\mathbf{u} + (1-\lambda)\boldsymbol{w} \in \mathcal{S} 
\end{equation*}


\section{The notion of regret}\label{section:theNotionOfRegret}

Convexity is crucial for the design of efficient algorithms. We consider the following online convex optimization framework\cite{shalev}: \\

\begin{figure}[ht]\centering
    \textit{Online Convex Optimization}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \KwInput{A $convex$ set $\mathcal{S}$}
        \For{$t = 1,2,\dots,$T} {
        predict strategy $\mathbf{w_t \in \mathcal{S}}$ \;
        receive a $convex$ loss function $f_t:\mathcal{S} \to \mathbb{R}$ \;
        suffer loss $f_t(\mathbf{w_t})$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}


Now, the \textit{regret} of an algorithm measures how "sorry" the learner is, to not have followed a fixed competing strategy in hindsight. It is the difference between the cumulative loss of the sequence of play and the cumulative loss of some fixed strategy. 


\begin{definition}\label{def:regret}
The \textit{regret} of an algorithm running for $T$ iterations relative to some competing strategy $\boldsymbol{u} \in \mathcal{S}$, is defined as
\[regret_T(\boldsymbol{u}) = \sum_{t=1}^{T}f_t(\boldsymbol{w}_t) - \sum_{t=1}^{T}f_t(\boldsymbol{u})\]
\end{definition}

Clearly, we want to minimize the \textit{regret} with respect to the best fixed strategy in hindsight. So relative to the set of competing strategies $\mathcal{S}$ the \textit{regret} of an online algorithm is defined as 

\begin{equation*}
    regret_T(\mathcal{S}) = \max_{\boldsymbol{u} \in \mathcal{S}} regret_T(\boldsymbol{u})
\end{equation*}

The learner's goal is to have the lowest \textit{regret} possible. In fact, we say an algorithm exhibits \textit{no-regret} if the difference between the cumulative loss of the sequence of play and the cumulative loss of the best fixed strategy tends to zero as $T$, the number of iterations, goes to infinity.

\begin{definition}\label{def:noRegret}
    An algorithm exhibits \textit{no-regret} iff $regret_T(\mathcal{S})$ grows sublinearly  with $T$, that is
    \[\text{\textit{no-regret}} \Leftrightarrow regret_T(\mathcal{S}) = o(T)\]
\end{definition}

\section{Follow the Regularized Leader}\label{section:followTheRegularizedLeader}

A natural learning rule is to use the one strategy, which had minimal loss on all past rounds. In formulas the update looks like this.

\begin{center}
    \textit{Follow the Leader (FTL)}  
\end{center}
\begin{equation*}
    \boldsymbol{w}_t = \argmin_{\boldsymbol{w} \in \mathcal{S}}\sum_{i=1}^{t-1}f_i(\boldsymbol{w}) \qquad \forall t  
\end{equation*} \\

In the first round we can play any feasible strategy and ties can be broken arbitrarily. While this approach yields \textit{no-regret} for some loss functions \cite[Cor. 2.2]{shalev}, FTL does not guarantee \textit{no-regret} in all cases. Consider the following counterexample as in \cite[Ex. 2.2]{shalev}. \\

Let $\mathcal{S} = [-1,1] \subset \mathbb{R}$ and consider the sequence of linear functions such that $f_t(w) = z_tw$ where

\begin{equation*}
    z_t = \begin{cases}
    -0.5 &\text{if $t = 1$}\\
    1 &\text{if $t$ is even}\\
    -1 &\text{if $t > 1 \land t$ is odd}
    \end{cases}
\end{equation*} \\

Apart from $t = 1$, where $z_t$ could actually be set arbitrarily in $[-1,1]$, the predictions of FTL will lead to $w_t = 1$ when $t$ is even and $w_t = -1$ when $t$ is odd. Therefore the cumulative loss of FTL will be $T$, while the cumulative loss of the fixed solution $u = 0 \in \mathcal{S}$ is $0$. Hence, the \textit{regret} of FTL is $T$, which is obviously not sublinearly with $T$. \\

In some sense, FTL seems to be "unstable". The predictions shift drastically from round to round. One way to stablize FTL is to add a so called regularization term. It makes sure that the prediction in the upcoming round is not too "far" off from the current one. 

\begin{definition}\label{def:regularizer}
    A function $R:\mathcal{S} \to \mathbb{R}$ is called regularization function if
    \begin{enumerate}
    \item $R$ is continuous
    \item $R$ is $\sigma-$strongly convex for some $\sigma > 0$
\end{enumerate}
\end{definition}

Using regularization we can define the Follow the Regularized Leader framework as follows

\begin{center}
    \textit{Follow the Regularized Leader (FoReL)}    
\end{center}
\begin{equation}\label{FoReL}
    \boldsymbol{w}_t = \argmin_{\boldsymbol{w} \in \mathcal{S}}\sum_{i=1}^{t-1}f_i(\boldsymbol{w}) + R(\boldsymbol{w}) \qquad \forall t
\end{equation} \\

In the upcoming subsections we will introduce two types of regularizations, the Euclidean and the entropic. Euclidean regularization will lead to what is known as Online Gradient Descent, whereas the entropic regularization provides an instance of Online Mirror Descent. Obviously, different regularization terms give different \textit{regret} bounds. We will discuss them later as well. 


\subsection{Online Gradient Descent}\label{subsection:onlineGradientAscent}

Consider the Online Linear Optimization problem where $f_t(\boldsymbol{w}) = \langle \boldsymbol{w},\boldsymbol{z}_t \rangle$ and let $\mathcal{S} = \mathbb{R}^d$. Applying Euclidean (or $l_2$) regularizer to FoReL will lead to Online Gradient Descent. 

\begin{definition}\label{def:euclideanReg}
    For some step size $\eta > 0$ the Euclidean (or $l_2$-) regularization term is defined as
    \[R(\boldsymbol{w}) = \frac{1}{2\eta}\|\boldsymbol{w}\|_2^2\]
\end{definition}
    

Rearranging the equation in \ref{FoReL} with Euclidean regularization yields the Online Gradient Descent algorithm as follows

\begin{figure}[H]\centering
    \textit{Online Gradient Descent (OGD)}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: }  step size $\eta > 0$ \;
        \textbf{initialize: } $\boldsymbol{w}_1 = \boldsymbol{0}$ \;
        \For{$t = 1,2,\dots,$T} {
        update $\boldsymbol{w}_{t+1} = \boldsymbol{w}_t - \eta\nabla f_t(\boldsymbol{w}_t)$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}

Intuitively, the OGD algorithm moves $\boldsymbol{w}_t$ in the direction of the minimum of $f_t$, but not too much because it wants to remember the effect of $f_1,\dots f_{t-1}$. This way, OGD is a \textit{no-regret} algorithm in as sense of definition \ref{def:noRegret} \cite[Cor. 2.7]{shalev}. 

\begin{proposition}\label{prop:EuclideanNoRegret}
    Assume that FoReL is run with Euclidean regularization on a sequence $f_1,\dots,f_T$ of convex functions. Further assume each $f_t$ is $L_t$-Lipschitz with respect to $\|\cdot\|_2$ and let $L$ be such that $\frac{1}{T}\sum_{t=1}^{T}{L_t}^2\le L^2$. Then, for $ \mathcal{S} = \{\boldsymbol{w}: \|\boldsymbol{w}\|_2 \le B\}$ and $\eta = \frac{B}{L\sqrt{2T}}$ we have that
    \[regret_T(\mathcal{S}) \le BL\sqrt{2T} \in o(T)\]
\end{proposition}

The problem with OGD is that it does not guarantee that the predictions $\boldsymbol{w}_t$ will always be in the probability simplex, the positive vectors whose elements sum to $1$. Therefore we cannot apply OGD in a Finite Game, where strategies are considered as probability vectors as we will see in Chapter \ref{chapter:finiteGames}. We somehow we have to \textit{project} the predictions back to the probability simplex in this case. In the next section we will discuss two ways to do so, namely Online Gradient Descent with Lazy Projections and Normalized Exponentiated Gradient. 


\subsection{Online Mirror Descent}\label{subsection:onlineMirrorAscent}

The idea of Online Mirror Descent is to find a link function that \textit{mirrors} the gradient step back to the feasible set. Lets define a link function $g: \mathbb{R}^d \to \mathcal{S}$ as 

\begin{equation}\label{link}
    g(\boldsymbol{\theta}) = \argmax_{\boldsymbol{w}} \langle\boldsymbol{w},\boldsymbol{\theta}\rangle - R(\boldsymbol{w})
\end{equation}

where $R(\boldsymbol{w})$ is the regularization term. We can rewrite FoReL from \ref{FoReL} based on the recursive update rule \cite{shalev}: 

\begin{center}
    \begin{tabular}{ll}
    \textbf{1.} & $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \nabla f(\boldsymbol{w}_t)$\\
    \textbf{2.} & $\boldsymbol{w}_{t+1} = g(\boldsymbol{\theta}_{t+1})$\\
    \end{tabular} 
\end{center}

This yields the Online Mirror Descent Framework. 

\begin{figure}[H]\centering
    \textit{Online Mirror Descent (OMD)}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: } a link function $g: \mathbb{R}^d \to \mathcal{S}$ \;
        \textbf{initialize: } $\boldsymbol{\theta}_1 = \boldsymbol{0}$ \;
        \For{$t = 1,2,\dots,$T} {
        predict strategy $\boldsymbol{w}_t = g(\boldsymbol{\theta}_t)$ \;
        update $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \nabla f(\boldsymbol{w}_t)$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}
Setting $\mathcal{S}= \mathbb{R}^d$ and $g(\boldsymbol{\theta}) = \eta\boldsymbol{\theta}$ we simply obtain Online Gradient Descent. So OMD is a generalization of OGD. \\

As we focus on Finite Games and mixed strategies later, we need the predictions to be probability vectors. Lets constrain $\mathcal{S}$ to be the probability simplex from now on and denote it by $\Delta$. It is defined as 

\begin{equation*}
    \Delta = \{\boldsymbol{w}: \|\boldsymbol{w}\|_1 = 1 \land \boldsymbol{w} \ge \boldsymbol{0}\}
\end{equation*}

In the following we will introduce two common link functions that mirror the predictions back to $\Delta$. 

\subsubsection{Online Gradient Descent with Lazy Projections}\label{subsubsection:OnlineGradientDescentWithLazyProjections}

Analogue to Online Gradient Descent we update the predictions of the algorithm at each time step moving in the negative direction of the gradient of the loss function. But this time we project the predictions back to the probability simplex using Euclidean projection. The link function $g(\boldsymbol{\theta})$ simply returns the closest point in $\Delta$ relative to the gradient step $\eta\boldsymbol{\theta}$. Formally, 

\begin{equation*}
    g(\boldsymbol{\theta}) = \argmin_{\boldsymbol{w} \in \Delta} \|\boldsymbol{w} - \eta\boldsymbol{\theta}\|_2
\end{equation*} \\

By applying this link function to the OMD framework we obtain an algorithm known as Online Gradient Descent with Lazy Projections:

\begin{figure}[H]\centering
    \textit{Online Gradient Descent \\ with Lazy Projections (OGDLP)} 
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: } step size $\eta > 0$ \;
        \textbf{initialize: } $\boldsymbol{\theta}_1 = \boldsymbol{0}$ \;
        \For{$t = 1,2,\dots,$T} {
        predict strategy $\boldsymbol{w}_t = \argmin_{\boldsymbol{w} \in \Delta} \|\boldsymbol{w} - \eta\boldsymbol{\theta}_t\|_2$ \;
        update $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \nabla f(\boldsymbol{w}_t)$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}

As shown in \cite[Cor. 2.17]{shalev} the Euclidean projection has no impact on the \textit{regret}. Since OGDLP is induced by Euclidean regularization it enjoys the same \textit{regret} bound as in proposition \ref{prop:EuclideanNoRegret} where $B = 1$ as we consider the feasible set to be the probability simplex. Consequently, OGDLP is considered as a \textit{no-regret} algorithm. The next algorithm yields the \textit{no-regret} property while having a much simpler update rule. Instead of Euclidean it uses entropic regularization.


\subsubsection{Normalized Exponentiated Gradient}\label{subsubsection:normalizedExponentiatedGradient}

Another suitable regularizer for simplex constrained optimization problems is the entropic regularization. It is defined as 

\begin{definition}\label{def:entropicReg}
    For some step size $\eta > 0$ the entropic regularization term is defined as
    \[R(\boldsymbol{w}) = \frac{1}{\eta} \sum_{i}w[i]log(w[i])\]
\end{definition}

where $log(\cdot)$ is the natural logarithm. Plugging in the entropic regularization in the link function from \ref{link} and rearranging yields the following mirror whose $i$-th component is the function \cite{shalev}

\begin{equation*}
    g_i(\boldsymbol{\theta}) = \frac{e^{\eta\theta[i]}}{\sum_{j}e^{\eta\theta[j]}}
\end{equation*} \\

According to the OMD framework we obtain the following simple update rule \cite{shalev}

\begin{equation*}
    \boldsymbol{w}_{t+1}[i] = \frac{w_t[i]e^{-\eta\nabla f_t[i]}}{\sum_{j}w_t[j]e^{-\eta\nabla f_t[j]}}
\end{equation*} \\

The derived algorithm is called Normalized Exponentiated Gradient. Note that we cannot initialize $\boldsymbol{w}_1 = \boldsymbol{0}$ as the entropic regularizer is not defined then. We need to choose some interior point of $\Delta$. \\

\begin{figure}[H]\centering
    \textit{Normalized Exponentiated Gradient (NEG)}
    \begin{minipage}{.7\linewidth}
        \begin{algorithm}[H]
        \DontPrintSemicolon
        \textbf{parameter: } step size $\eta > 0$ \;
        \textbf{initialize: } $\{\boldsymbol{w}: \|\boldsymbol{w}\|_1 = 1 \land \boldsymbol{w} > \boldsymbol{0}\}$ \; 
        \For{$t = 1,2,\dots,$T} { 
        update $\boldsymbol{w}_{t+1}[i] = \scaleto{\frac{w_t[i]e^{-\eta\nabla f_t[i]}}{\sum_{j}w_t[j]e^{-\eta\nabla f_t[j]}}}{27pt} \quad \forall i$ \;
        }
        \end{algorithm}\caption*{}
  \end{minipage}
\end{figure}

Just like OGDLP, NEG is a \textit{no-regret} algorithm in the sense of definition \ref{def:noRegret} \cite[Cor. 2.14]{shalev}.

\begin{proposition}\label{prop:EntropicNoRegret}
    Assume that FoReL is run with entropic regularization on a sequence $f_1,\dots,f_T$ of convex functions. Further assume each $f_t$ is $L_t$-Lipschitz with respect to $\|\cdot\|_1$ and let $L$ be such that $\frac{1}{T}\sum_{t=1}^{T}{L_t}^2\le L^2$. Then, for $ \mathcal{S} = \{\boldsymbol{w}: \|\boldsymbol{w}\|_2 \le B\}$ and $\eta = \frac{B}{L\sqrt{2T}}$ we have that
    \[regret_T(\mathcal{S}) \le BL\sqrt{2log(d)T} \in o(T)\]
\end{proposition}

Note that in comparison with the \textit{regret} bound of OGDLP as in proposition \ref{prop:EuclideanNoRegret} the \textit{regret} bound of NEG also depends on the dimension of the predictions. As we constrained on the probability simplex we can set $B = 1$. \\

To sum up, we introduced two \textit{no-regret} Algorithms that can be applied for simplex constrained convex optimization problems. First, Online Gradient Descent with Lazy Projections (OGDLP) which was based on Euclidean regularization and second, Normalized Exponentiated Gradient (NEG) induced by entropic regularization. We will run both algorithms in simple finite games later in chapter \ref{chapter:simulations}. Before that, lets define finite games and their equilibira concepts. 

