
\chapter{Introduction}\label{chapter:introduction}

\begin{comment}
\begin{itemize}
    \item So far, there is no comprehensive characterization of games that are “learn- able,” but there are some important results. For example, it is well-known that no-regret dynamics converge to a coarse correlated equilibrium in arbitrary fi- nite games [17,26,15,12]. Coarse correlated equilibria (CCE) encompass the set of correlated equilibria (CE) of a finite game. The latter is a nonempty convex polytope which in turn contains the convex hull of the game’s Nash equilibria such that we get NE subset CE subset CCE. (ComputingBNE, p.3)
    \item investigate the question whether NE is learnable via nor regret dynamics \cite{jafari} Introduction
    \item What is the outcome of multi agent learning via no regret algorithms in repeated games? A learning algorithm is said to exhibit no regret iff average payoffs that are achieved by the algorithm exceed tha payoffs that could be achieved by any fixed strategy. in the limit. \cite{jafari} Introduction
    \item best response dynamics like fictitious play does not converge to NE in general  because rational play yields deterministic play. Therefore rational play can not possibly converge to NE in games where no PNE exist. In contrast no regret dynamics are recipes by which to update probabilities that agents assaign to actions could potentially learn MNEs. 
    \item under FoReL only \textit{strict Nash equilibria} survive. 
    
    \item maybe divide in Problem and Contribution
    
    \item There is an unfortunate disconnect between game
    theory and optimization in terms of how objectives are formulated. In optimization,
    the objective is to minimize the incurred cost; in game theory, to maximize one’s rewards. In turn, this creates a clash of terminology when referring to methods such as
    “gradient descent” or “mirror descent” in a maximization setting. To avoid going against
    the traditions of each field, we keep the usual conventions in place (minimization in
    optimization, maximization in game theory), and we rely on the reader to make the
    mental substitution of “descent” to “ascent” when needed (merthabili p.4)
    
    \item we focus on learning via “dual averaging”, a widely used class of no-regret learning schemes where players take small steps along their individual payoff gradients and then “mirror” the output back to their action sets \cite[abstract]{mertikopoulos}
\end{itemize}
\end{comment}

Some Latex stuff: \\







