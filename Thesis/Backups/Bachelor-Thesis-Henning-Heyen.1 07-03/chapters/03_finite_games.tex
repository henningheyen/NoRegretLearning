
\chapter{Finite Games}\label{chapter:finiteGames}

In a mathematical context games are models of strategic behavior between players that have individual interests. All games have the same basic structure. They consist of players, each player's set of actions and the some payoff that depends on the players' actions. Just by defining these three element we can derive well-defined solution concepts such as Nash equilibria. The fundamental question of this thesis is: If all player choose theirs actions according to a \textit{no-regret} update policy in a repeated game, will the outcome be some kind of equilibrium state. Before we can answer that question let us introduce the concept of games and equilibria more formally.


\section{Notation and Definitions}\label{section:notationAndDefinitionsGames}

Formally, a finite game is a tuple $\mathcal{G} \equiv (\mathcal{N}, {(\mathcal{X}_i})_{i\in\mathcal{N}},{(u_i)}_{i\in\mathcal{N}})$ where each \textit{player} $i \in \mathcal{N} = \{1,\dots,N\}$ chooses an \textit{action} $x_i$ from a compact convex subset $\mathcal{X}_i$ of a finite-dimnesional normed space $\mathcal{V}_i$. Let $\mathcal{X} = \prod_{i}\mathcal{X}_i$ be the \textit{action space}. The players' \textit{payoff}, also called \textit{utility} or \textit{reward}, depends on all players' \textit{actions}. An \textit{action profile} is denoted by $x = (x_1,\dots,x_N) \in \mathcal{X}$. When we want to highlight that \textit{player} $i$ selects \textit{action} $x_i$ we write $x = (x_i,x_{-i})$ where $x_{-i} = {(x_j)}_{j\neq i}$ is the ensemble of actions selected by the other \textit{players}. The \textit{players' payoff} is determined by a \textit{payoff function} $u_i : \mathcal{X} \to \mathbb{R} $. Assuming $u_i$ to be continuously differentiable we write 

\begin{equation*}
    v_i(x) = \nabla_{x_i}u_i(x_i,x_{-i})
\end{equation*}

for the \textit{individual gradient} of $u_i$ at $x$. As as special case we consider payoff functions that are \textit{individually concave} in a sense that 

\begin{equation}\label{concaveGame}
    u_i(x_i,x_{-i}) \textit{ is concave in $x_i$ for all $x_{-i} \in \prod_{j\neq i}\mathcal{X}_j,i \in \mathcal{N}$}
\end{equation}

When this is the case we say that the game itself is \textit{concave}. A well known version of a such a game is a \textit{finite game with mixed extensions}. \\

In finite games with mixed extensions we allow players to "mix" the actions they want to play. So the strategies that players choose are probability vectors. Formally in a \textit{mixed extension} of $\Gamma \equiv (\mathcal{N}, {(\mathcal{A}_i})_{i\in\mathcal{N}},{(u_i)}_{i\in\mathcal{N}})$ each \textit{player} $i \in \mathcal{N}$ chooses an \textit{action} $a_i$ from a finite set $\mathcal{A}_i$ of \textit{pure strategies}. Let $\mathcal{A} = \prod_{i}\mathcal{A}_i$ be the \textit{action space} of pure strategies. There are no assumptions on the \textit{players' payoff} function of pure strategies $u_i: \mathcal{A} \to \mathbb{R}$. Players can also play \textit{mixed strategies}, i.e probability distributions $x_i$ drawn from the probability simplex $\mathcal{X}_i = \Delta(\mathcal{A}_i)  = \{x_i: \sum_{a_i\in \mathcal{A}_i}x_{i,a_i} = 1 \land x_i \ge 0\}$. In that case the expected payoff of some player $i$ depends on a \textit{mixed profile} $x = (x_1,\dots,x_N) \in \mathcal{X} = \prod_{i}\mathcal{X}_i$ and can be written as 

\begin{equation*}
    u_i(x) = \sum_{a_1\in\mathcal{A}_1}\dots\sum_{a_N\in\mathcal{A}_N} x_{1,a_1} \dots x_{N,a_N}u_i(a_1,\dots,a_N)
\end{equation*}

Then the \textit{players' individual gradients} are simply their \textit{payoff} vectors 

\begin{equation*}
    v_i(x) = \nabla_{x_i}u_i(x) = (u_i(a_i,x_{-i}))_{a_i\in\mathcal{A}_i}
\end{equation*}

Because $\mathcal{X}_i = \Delta(\mathcal{A}_i)$ is convex and $u_i$ is linear in $x_i$, the mixed extension of $\Gamma$ is concavein the sense of equation \ref{concaveGame}. \\

The game is said to be \textit{zero sum} if for all pure action profiles $a \in \mathcal{A}$ the cumulative utilities of all players sum up to zero.  

\begin{equation*}
    \sum_{i = 1}^{N} u_i(a)= 0 \qquad \forall a \in \mathcal{A} \quad (\textit{zero sum})
\end{equation*}

In the literature we sometimes find \textit{constant sum} games, i.e. the equation above holds for some constant $c \in \mathbb{R}$. However, constant sum games are equivalent to zero sum games as they can be normalized. By simply subtracting $\frac{c}{N}$ from all utilities we obtain a zero sum game without any loss of information. If a game is neither zero nor constant sum we say it is a \textit{general sum} game.\\

Assume the mixed extension of $\Gamma$ is played over and over again and call this a \textit{repeated game} $\Gamma^{\infty}$. Consider $x^t = (x_{1}^{t},\dots,x_{N}^{t})$ as the strategy profile that was played in round $t$ where each $x_{i}^{t} \in \Delta(\mathcal{A}_i)$ denotes player $i$'s mixed strategy in round $t$. We can apply the concepts from chapter \ref{chapter:noRegretLearning} where each player $i \in \mathcal{N}$ predicts a mixed strategy $x_{i}^{t}$ in each iteration step $t$ simultaneously. Then the players receive feedback in form of the players' individual gradients $v_{i}(x^t) = \nabla_{x_{i}^{t}}u_{i}(x^t)$. \\

Unfortunately there is a misleading difference between \textit{game theory} and \textit{optimization} in terms of how the objective is formulated. In optimization agents aim to \textit{minimize} losses. In game theory, however, the general convention is that players \textit{maximize} utilities. Of course, we can easily convert a minimizing problem into an equivalent minimizing one by taking the negative objective. That is also why we have to assume the utility function to be concave instead of convex. The notion of regret as in definition \ref{def:regret} can be reformulated into a game theoretic context. The \textit{regret} of player $i$ is then defined as

\begin{equation*}
    reg{i}^(T) = \max_{x_i \in \Delta}\sum_{t=1}^{T} u_i(x_i,x_{-i}^{t}) - \sum_{t=1}^{T}u_i(x^t)
\end{equation*}

Also note that the derived algorithms in chapter \ref{chapter:noRegretLearning} were \textit{descent} algorithms. As we are maximizing utilities now, they become \textit{ascent} algorithms. In particular, we move in the positive direction of the gradient. For that reason \textit{projected online gradient descent} (\ref{equ:POGD}) will be named \textit{online projected gradient ascent} (OPGA) hereafter. Similarly, \textit{entropic gradient descent} (\ref{equ:EGD}) is now called \textit{entropic gradient ascent} (EGA). \\

Since the probability simplex $\Delta(\mathcal{A}_i)$ is convex and $u_i$ is linear, which is concave, we can apply these algorithms to \textit{repeated games} and hope that the sequence of play will converge to some notion of equilibrium. Equilibria concepts are discussed in more detail in section \ref{section:equilibriaConcepts}. Before that, lets introduce a well known example of a finite game with mixed extensions in order to have a better intuition. 


\section{An Example}\label{section:anExample}

For two player games the easiest way to define it is by drawing its \textit{payoff matrix}. For a better understanding lets consider an example. The zero sum game Rock Paper Scissors, for instance, has the following payoff matrix.\\

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Rock$}  & \multicolumn{1}{c}{$Paper$}  & \multicolumn{1}{c}{$Scissors$} \\\cline{3-5}
            & $Rock$ & $0,0$ & $-1,1$ & $1,-1$ \\ \cline{3-5}
            & $Paper$ & $1,-1$ & $0,0$ & $-1,1$ \\\cline{3-5}
            & $Scissors$ & $-1,1$ & $1,-1$ & $0,0$ \\\cline{3-5}
\end{tabular}\caption{\label{tab:payoffRPSFromDefinition}payoff matrix Rock Paper Scissors}
\end{table}

Obviously $\mathcal{N} = \{1,2\}$. Following the convention, we will call payer $1$ the \textit{row player} and player $2$ the \textit{column player} respectively. Both have the same action space $\mathcal{A}_1 = \mathcal{A}_2 = \{\textit{Rock,Paper,Scissor}\} \equiv \{\textit{R,P,S\}}$. Lets abbreviate the actions due to readability. The utilities for pure strategies are given in the payoff matrix, where the first entries refer to the row player and the second entries to the column player. So for example, when the row player selects \textit{Paper} while the column player chooses \textit{Rock} then the row player wins and the column player looses. In formulas

\begin{equation*}
    u_1(\textit{P,R}) = 1 \qquad \text{and} \qquad u_2(\textit{P,R}) = -1
\end{equation*}

We can use matrix notation and write the utility for mixed strategies $x = (x_1,x_2)$ as

\begin{equation*}
    u_1(x) = x{_1}^{T}Ax_2 \qquad \text{and} \qquad u_2(x) = x{_1}^{T}Bx_2
\end{equation*}

where A,B are the pure strategy payoffs matrices for each player derived by table \ref{tab:payoffRPSFromDefinition}.

\begin{equation*}
A = \begin{pmatrix}
0 & -1 & 1\\
1 & 0 & -1\\
-1 & 1 & 0
\end{pmatrix}
\qquad \text{and} \qquad
B = \begin{pmatrix}
0 & 1 & -1\\
-1 & 0 & 1\\
1 & -1 & 0
\end{pmatrix}
\end{equation*}

We can think of \textit{mixed strategies} as probability vectors where players assign weights on actions. So for example the row player can choose to play \textit{Rock} and \textit{Paper} equally likely and not to play \textit{Scissors}. In formulas, $x_1 = (1/2,1/2,0)$. When the row player chooses to play solely rock, i.e $x_2 = (1,0,0)$ then the expected payoff for both players would be

\begin{equation*}
    u_1(x) = 1/2  \qquad \text{and} \qquad u_2(x) = -1/2
\end{equation*}

Note that strategies where a single action is assigned with probability 1 and the others with probability 0 are called \textit{pure strategies} and \textit{fully mixed strategies} otherwise. Both are are feasible \textit{mixed strategies} though. \\ 

Finally the individual gradients for both players are simply their payoff vectors.

\begin{equation*}
    v_1(x) = \nabla_{x_1}u_1(x) = Ax_2 \qquad \text{and} \qquad v_2(x) = \nabla_{x_2}u_2(x) = x_{1}^{T}B
\end{equation*}


\section{Equilibria Concepts}\label{section:equilibriaConcepts}

\subsection{Pure and Mixed Nash Equilibria}\label{subsection:PNEandMNE}

The most widely used solution concept in game theory is that of a \textit{Nash equilibrium} (NE). It is a state, where no player can increase their own expected payoff by changing their strategy while the other players keep their strategy unchanged. In other words, Nash equilibria are those strategy profiles that give no player the incentive to unilaterally deviate. In finite games with mixed extentions we can we can distinguish between mixed and pure Nash equilibria. 

\begin{definition}\label{def:MNE}
A \textit{mixed strategy profile} $x^* \in \mathcal{X}$ is called \textit{mixed Nash equilibrium} (MNE) if
    \[u_i(x_{i}^{*},x_{-i}^{*}) \le  u_i(x_{i},x_{-i}^{*}) \qquad \forall x_i \in \mathcal{X}_i, i \in \mathcal{N}\]
\end{definition}

For instance, the unique MNE in Rock Paper Scissors from section \ref{section:anExample} would be that each player chooses all actions equally likely, i.e $x_{i}^{*} = (1/3,1/3,1/3) \ \forall i \in \mathcal{N}$. A special case of MNE is when all players choose pure strategies. Remember a pure strategy simply means that all player $i \in \mathcal{N}$ select one action $a_i \in \mathcal{A}_i$ with probability 1 and the other actions $a_j \in \mathcal{A}_i \setminus \{a_i\}$ with probability 0. We can then define a \textit{pure Nash equilibria} as follows.

\begin{definition}\label{def:PNE}
    An action profile $a^* \in \mathcal{A}$ is called pure Nash equilibria (PNE) if
    \[u_i(a_{i}^{*},a_{-i}^{*}) \le u_i(a_{i},a_{-i}^{*}) \qquad \forall a_i \in \mathcal{A}_i, i \in \mathcal{N}\]
\end{definition}

Cleary, all pure Nash equilibria are also mixed Nash equilibria. All $x_{i}^{*}$ are simply unit vectors then. When we think of PNE and MNE as sets then we have 

\begin{equation*}
    PNE \subset MNE
\end{equation*}

If a Nash equilibrium is not pure we call this an \textit{interior} or \textit{fully mixed} Nash equilibrium. We can further distinguish between \textit{strict} and \textit{weak Nash equilibria}. A Nash equilibrium $x^*$ is called strict when definition \ref{def:MNE} holds with strict inequality for all $x_i \neq x_{i}^{*}$. In other words, $x^*$ is strict if no player can deviate unilaterally from $x^*$ without \textit{reducing} their payoff or, equivalently, when every player has a unique best response to $x^*$. This implies that strict Nash equilibria are pure strategy profiles $x^* = (a_{1}^{*},\dots,a_{N}^{*})$ such that

\begin{definition}\label{def:strictNE}
    A pure strategy profile $x^* = (a_{1}^{*},\dots,a_{N}^{*})$ is called strict Nash equilibrium if
    \[u_i(a_{i}^{*},a_{-i}^{*}) < u_i(a_{i},a_{-i}^{*}) \qquad \forall a_i \in \mathcal{A}_i \setminus \{a_{i}^{*}\} , i \in \mathcal{N}\]
\end{definition}

If a NE is not strict we will call it \textit{weak}. Whether or not a NE is strict plays an important role when it comes to the convergence behaviour of \textit{no-regret} algorithms. We will also discuss the tractability of computing Nash equilibria. It turns out that its hard. But more on that later in chapter \ref{chapter:literatureReview}. Before that, a less common but in fact computationally tractable solution concept is introduced, namely \textit{correlated} and \textit{coarse correlated equilibria}.\\

\subsection{Correlated and Coarse Correlated Equilibria}\label{subsection:CEandCCE}

Consider the following example\footnote{University of Pennsylvania, Prof. Aaron Roth, NETS 412 Algorithmic Game Theory, Lecture 8}

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $0,0$ & $0,1$ \\\cline{3-4}
  & $Go$ & $1,0$ & $-100,-100$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:payoffIntersectionfromFiniteGames}payoff matrix Intersection Game}
\end{table}

Imagine an intersection where the players are drivers that can either \textit{Stop} or \textit{Go}. In short, $\mathcal{A}_i = \{S,G\}$ for both players. The players' goal is to \textit{Go} without a crash. When both play \textit{Go} they crash and when both play \textit{Stop} no one gains any utility. The payoff is set accordingly as in table \ref{tab:payoffIntersectionfromFiniteGames}. \\

There are two pure Nash equilibria, $(S,G)$ and $(G,S)$. Note that at least one player has payoff $0$ in that case. There is also a mixed Nash equilibrium. Suppose the row player selects the strategy $x_1 = (p,1-p)$. Then the column player must be indifferent between both actions, i.e 

\begin{equation*}
    0 = p - 100(1-p) \iff 101p = 100 \iff p = 100/101
\end{equation*}

So both players choosing \textit{Stop} with probability $p = 100/101$ and \textit{Go} with probability $1-p = 1/101$  leads to a MNE. In terms of utility, this is even worse as the expected payoff for both players is $0$. Under the MNE the four possible action
profiles have roughly the following probabilities. 

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $98\%$ & $<1\%$ \\\cline{3-4}
  & $Go$ & $<1\%$ & $\approx 0.01\%$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:probabilityUnderMNE}action profile probability distribution under MNE}
\end{table}

A much better outcome would the following distribution

\begin{table}[H]\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{cc|c|c|}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$Stop$}  & \multicolumn{1}{c}{$Go$} \\\cline{3-4}
  & $Stop$ & $0\%$ & $50\%$ \\\cline{3-4}
  & $Go$ & $50\%$ & $0\%$ \\\cline{3-4}
\end{tabular}\caption{\label{tab:probabilityUnderCE}action profile probability distribution under CE}
\end{table}

Both player have expected utility $1/2$ and don't have to risk a crash. The problem is that there is no MNE that yields this probability distribution. The reason is that Nash equilibria are defined as profiles of mixed strategies, that require players to randomize independently, without any
communication. The above distribution, however, requires both to correlate their actions. On real streets that correlation device is a traffic light. It suggests both driver whether to \textit{Stop} or to \textit{Go}. Following its advice is a best response for everyone. That concept can be formalized. 

\begin{definition}\label{def:CE}
    A \textit{correlated equilibrium} (CE) is a distribution $\mathcal{D}$ over action profiles $\mathcal{A}$ such that for all player $i \in \mathcal{N}$ and \textit{action} $a_{i}^{*} \in \mathcal{A}_i$
    
    \[\mathbb{E}_{a \sim \mathcal{D}}[u_i(a)] \ge \mathbb{E}_{a \sim \mathcal{D}}[u_i(a_{i}^{*},a_{-i})|a_i]\]
\end{definition}

In words, a CE is a probability distribution over action profiles such that after a profile $a$ is drawn from this distribution, playing $a_i$ is a best response for player $i$ conditioned on seeing $a_i$, given that all the other players play according to $a$. In the intersection game, for instance, conditioned on seeing \textit{Stop}, playing \textit{Stop} is indeed a best response given that the other player sees \textit{Go}. Likewise, conditioned on seeing \textit{Go}, playing \textit{Go} is indeed a best response given that the other player sees \textit{Stop}. \\

Nash equilibira are also correlated equilibria. It's just that the players' actions are drawn from an independent distribution, so being conditioned on $a_i$ provides no additional information to $a_{-i}$. Therefore the set of CE strictly contains the set of MNE. An even larger solution concept is the set of \textit{coarse correlated equilibria} (CCE). 

\begin{definition}
    A CCE is a distribution $\mathcal{D}$ over action profiles $\mathcal{A}$ such that for all player $i \in \mathcal{N}$ and \textit{action} $a_{i}^{*} \in \mathcal{A}_i$

    \[\mathbb{E}_{a \sim \mathcal{D}}[u_i(a)] \ge \mathbb{E}_{a \sim \mathcal{D}}[u_i(a_{i}^{*},a_{-i})]\]
\end{definition}

The difference to CE is that CCE only requires that following a suggested action $a_i$ when $a$ is drawn from $\mathcal{D}$ is only a best response \textit{before} $a_i$ is seen. One can show that the set of CCE is strictly larger than the set of CE by constructing an distribution that is a CCE but not a CE\footnote{University of Pennsylvania, Prof. Aaron Roth, NETS 412 Algorithmic Game Theory, Lecture 8}. That yields the following equilibria hierarchy

\begin{equation*}
    PNE \subset MNE \subset CE \subset CCE
\end{equation*}

Even though CCE lead to higher expected payoffs they may contain strictly dominated pure strategy profiles with positive probability, so they fail the most basic assumption that players are rational. \cite{viossat}. This means that CCE is a rather weak solution concept. Even though hard to compute, Nash equilibrium is still the most robust and stable solution concept. To what extent do \textit{no-regret dynamics} converge to these solution concepts?  That will be discussed in the next chapter.


